{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "from tokenizers import decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"bert_ner_finetuned_iliad-with-gpu.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['O', 'B-CLEntity', 'I-CLEntity', 'L-CLEntity', 'U-CLEntity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"\"\"The man for wisdom’s various arts renown’d,\n",
    "Long exercised in woes, O Muse! resound;\n",
    "Who, when his arms had wrought the destined fall\n",
    "Of sacred Troy, and razed her heaven-built wall,\n",
    "Wandering from clime to clime, observant stray’d,\n",
    "Their manners noted, and their states survey’d,\n",
    "On stormy seas unnumber’d toils he bore,\n",
    "Safe with his friends to gain his natal shore:\n",
    "Vain toils! their impious folly dared to prey\n",
    "On herds devoted to the god of day;\n",
    "The god vindictive doom’d them never more\n",
    "(Ah, men unbless’d!) to touch that natal shore.\n",
    "Oh, snatch some portion of these acts from fate,\n",
    "that's Celestial Muse! and to our world relate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
    "inputs = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(inputs).logits\n",
    "predictions = torch.argmax(outputs, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'The', 'man', 'for', 'wisdom', '’', 's', 'various', 'arts', 're']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS]', 'LABEL_0')\n",
      "('The', 'LABEL_0')\n",
      "('man', 'LABEL_0')\n",
      "('for', 'LABEL_0')\n",
      "('wisdom', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('s', 'LABEL_0')\n",
      "('various', 'LABEL_0')\n",
      "('arts', 'LABEL_0')\n",
      "('re', 'LABEL_0')\n",
      "('##no', 'LABEL_0')\n",
      "('##wn', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('Long', 'LABEL_4')\n",
      "('exercised', 'LABEL_0')\n",
      "('in', 'LABEL_0')\n",
      "('w', 'LABEL_0')\n",
      "('##oes', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('O', 'LABEL_0')\n",
      "('Muse', 'LABEL_4')\n",
      "('!', 'LABEL_0')\n",
      "('re', 'LABEL_0')\n",
      "('##sound', 'LABEL_0')\n",
      "(';', 'LABEL_0')\n",
      "('Who', 'LABEL_4')\n",
      "(',', 'LABEL_0')\n",
      "('when', 'LABEL_0')\n",
      "('his', 'LABEL_0')\n",
      "('arms', 'LABEL_0')\n",
      "('had', 'LABEL_0')\n",
      "('wrought', 'LABEL_0')\n",
      "('the', 'LABEL_0')\n",
      "('destined', 'LABEL_0')\n",
      "('fall', 'LABEL_0')\n",
      "('Of', 'LABEL_0')\n",
      "('sacred', 'LABEL_0')\n",
      "('Troy', 'LABEL_4')\n",
      "(',', 'LABEL_0')\n",
      "('and', 'LABEL_0')\n",
      "('r', 'LABEL_0')\n",
      "('##azed', 'LABEL_0')\n",
      "('her', 'LABEL_0')\n",
      "('heaven', 'LABEL_0')\n",
      "('-', 'LABEL_0')\n",
      "('built', 'LABEL_0')\n",
      "('wall', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('Wan', 'LABEL_1')\n",
      "('##dering', 'LABEL_3')\n",
      "('from', 'LABEL_0')\n",
      "('c', 'LABEL_0')\n",
      "('##lim', 'LABEL_0')\n",
      "('##e', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('c', 'LABEL_0')\n",
      "('##lim', 'LABEL_0')\n",
      "('##e', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('o', 'LABEL_0')\n",
      "('##bs', 'LABEL_0')\n",
      "('##er', 'LABEL_0')\n",
      "('##vant', 'LABEL_0')\n",
      "('stray', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('Their', 'LABEL_4')\n",
      "('manners', 'LABEL_0')\n",
      "('noted', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('and', 'LABEL_0')\n",
      "('their', 'LABEL_0')\n",
      "('states', 'LABEL_0')\n",
      "('survey', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('On', 'LABEL_4')\n",
      "('storm', 'LABEL_0')\n",
      "('##y', 'LABEL_0')\n",
      "('seas', 'LABEL_0')\n",
      "('un', 'LABEL_0')\n",
      "('##num', 'LABEL_0')\n",
      "('##ber', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('##ils', 'LABEL_0')\n",
      "('he', 'LABEL_0')\n",
      "('bore', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('Safe', 'LABEL_4')\n",
      "('with', 'LABEL_0')\n",
      "('his', 'LABEL_0')\n",
      "('friends', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('gain', 'LABEL_0')\n",
      "('his', 'LABEL_0')\n",
      "('na', 'LABEL_0')\n",
      "('##tal', 'LABEL_0')\n",
      "('shore', 'LABEL_0')\n",
      "(':', 'LABEL_0')\n",
      "('V', 'LABEL_1')\n",
      "('##ain', 'LABEL_3')\n",
      "('to', 'LABEL_0')\n",
      "('##ils', 'LABEL_0')\n",
      "('!', 'LABEL_0')\n",
      "('their', 'LABEL_0')\n",
      "('imp', 'LABEL_0')\n",
      "('##ious', 'LABEL_0')\n",
      "('f', 'LABEL_0')\n",
      "('##olly', 'LABEL_0')\n",
      "('dared', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('prey', 'LABEL_0')\n",
      "('On', 'LABEL_0')\n",
      "('herd', 'LABEL_0')\n",
      "('##s', 'LABEL_0')\n",
      "('devoted', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('the', 'LABEL_0')\n",
      "('god', 'LABEL_0')\n",
      "('of', 'LABEL_0')\n",
      "('day', 'LABEL_0')\n",
      "(';', 'LABEL_0')\n",
      "('The', 'LABEL_4')\n",
      "('god', 'LABEL_0')\n",
      "('v', 'LABEL_0')\n",
      "('##ind', 'LABEL_0')\n",
      "('##ict', 'LABEL_0')\n",
      "('##ive', 'LABEL_0')\n",
      "('doom', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "('them', 'LABEL_0')\n",
      "('never', 'LABEL_0')\n",
      "('more', 'LABEL_0')\n",
      "('(', 'LABEL_0')\n",
      "('Ah', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('men', 'LABEL_0')\n",
      "('un', 'LABEL_0')\n",
      "('##bles', 'LABEL_0')\n",
      "('##s', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "('!', 'LABEL_0')\n",
      "(')', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('touch', 'LABEL_0')\n",
      "('that', 'LABEL_0')\n",
      "('na', 'LABEL_0')\n",
      "('##tal', 'LABEL_0')\n",
      "('shore', 'LABEL_0')\n",
      "('.', 'LABEL_0')\n",
      "('Oh', 'LABEL_4')\n",
      "(',', 'LABEL_0')\n",
      "('s', 'LABEL_0')\n",
      "('##natch', 'LABEL_0')\n",
      "('some', 'LABEL_0')\n",
      "('portion', 'LABEL_0')\n",
      "('of', 'LABEL_0')\n",
      "('these', 'LABEL_0')\n",
      "('acts', 'LABEL_0')\n",
      "('from', 'LABEL_0')\n",
      "('fate', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('that', 'LABEL_0')\n",
      "(\"'\", 'LABEL_0')\n",
      "('s', 'LABEL_0')\n",
      "('Ce', 'LABEL_1')\n",
      "('##les', 'LABEL_2')\n",
      "('##tial', 'LABEL_3')\n",
      "('Muse', 'LABEL_4')\n",
      "('!', 'LABEL_0')\n",
      "('and', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('our', 'LABEL_0')\n",
      "('world', 'LABEL_0')\n",
      "('relate', 'LABEL_0')\n",
      "('.', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n"
     ]
    }
   ],
   "source": [
    "for token, prediction in zip(tokens, predictions[0].numpy()):\n",
    "     print((token, model.config.id2label[prediction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern = r' ([A-Z].[a-z]+)'\n",
    "pattern = r'(\\b[A-Z][a-z]+\\b)(\\s\\b[A-Z][a-z]+\\b)*'\n",
    "re.compile(pattern)\n",
    "\n",
    "def get_annotations(text, pattern):\n",
    "    annotations = []\n",
    "    for match in re.finditer(pattern, text):\n",
    "        label_dic = dict()\n",
    "        label_dic['start'] = match.start()\n",
    "        label_dic['end'] = match.end()\n",
    "        label_dic['label'] = 'CLEntity' # Entity starting with a capital letter\n",
    "        annotations.append(label_dic)\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_annotations = get_annotations(sequence, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] - O\n",
      "The - U-CLEntity\n",
      "man - O\n",
      "for - O\n",
      "wisdom - O\n",
      "’ - O\n",
      "s - O\n",
      "various - O\n",
      "arts - O\n",
      "re - O\n",
      "##no - O\n",
      "##wn - O\n",
      "’ - O\n",
      "d - O\n",
      ", - O\n",
      "Long - U-CLEntity\n",
      "exercised - O\n",
      "in - O\n",
      "w - O\n",
      "##oes - O\n",
      ", - O\n",
      "O - O\n",
      "Muse - U-CLEntity\n",
      "! - O\n",
      "re - O\n",
      "##sound - O\n",
      "; - O\n",
      "Who - U-CLEntity\n",
      ", - O\n",
      "when - O\n",
      "his - O\n",
      "arms - O\n",
      "had - O\n",
      "wrought - O\n",
      "the - O\n",
      "destined - O\n",
      "fall - O\n",
      "Of - U-CLEntity\n",
      "sacred - O\n",
      "Troy - U-CLEntity\n",
      ", - O\n",
      "and - O\n",
      "r - O\n",
      "##azed - O\n",
      "her - O\n",
      "heaven - O\n",
      "- - O\n",
      "built - O\n",
      "wall - O\n",
      ", - O\n",
      "Wan - B-CLEntity\n",
      "##dering - L-CLEntity\n",
      "from - O\n",
      "c - O\n",
      "##lim - O\n",
      "##e - O\n",
      "to - O\n",
      "c - O\n",
      "##lim - O\n",
      "##e - O\n",
      ", - O\n",
      "o - O\n",
      "##bs - O\n",
      "##er - O\n",
      "##vant - O\n",
      "stray - O\n",
      "’ - O\n",
      "d - O\n",
      ", - O\n",
      "Their - U-CLEntity\n",
      "manners - O\n",
      "noted - O\n",
      ", - O\n",
      "and - O\n",
      "their - O\n",
      "states - O\n",
      "survey - O\n",
      "’ - O\n",
      "d - O\n",
      ", - O\n",
      "On - U-CLEntity\n",
      "storm - O\n",
      "##y - O\n",
      "seas - O\n",
      "un - O\n",
      "##num - O\n",
      "##ber - O\n",
      "’ - O\n",
      "d - O\n",
      "to - O\n",
      "##ils - O\n",
      "he - O\n",
      "bore - O\n",
      ", - O\n",
      "Safe - U-CLEntity\n",
      "with - O\n",
      "his - O\n",
      "friends - O\n",
      "to - O\n",
      "gain - O\n",
      "his - O\n",
      "na - O\n",
      "##tal - O\n",
      "shore - O\n",
      ": - O\n",
      "V - B-CLEntity\n",
      "##ain - L-CLEntity\n",
      "to - O\n",
      "##ils - O\n",
      "! - O\n",
      "their - O\n",
      "imp - O\n",
      "##ious - O\n",
      "f - O\n",
      "##olly - O\n",
      "dared - O\n",
      "to - O\n",
      "prey - O\n",
      "On - U-CLEntity\n",
      "herd - O\n",
      "##s - O\n",
      "devoted - O\n",
      "to - O\n",
      "the - O\n",
      "god - O\n",
      "of - O\n",
      "day - O\n",
      "; - O\n",
      "The - U-CLEntity\n",
      "god - O\n",
      "v - O\n",
      "##ind - O\n",
      "##ict - O\n",
      "##ive - O\n",
      "doom - O\n",
      "’ - O\n",
      "d - O\n",
      "them - O\n",
      "never - O\n",
      "more - O\n",
      "( - O\n",
      "Ah - U-CLEntity\n",
      ", - O\n",
      "men - O\n",
      "un - O\n",
      "##bles - O\n",
      "##s - O\n",
      "’ - O\n",
      "d - O\n",
      "! - O\n",
      ") - O\n",
      "to - O\n",
      "touch - O\n",
      "that - O\n",
      "na - O\n",
      "##tal - O\n",
      "shore - O\n",
      ". - O\n",
      "Oh - U-CLEntity\n",
      ", - O\n",
      "s - O\n",
      "##natch - O\n",
      "some - O\n",
      "portion - O\n",
      "of - O\n",
      "these - O\n",
      "acts - O\n",
      "from - O\n",
      "fate - O\n",
      ", - O\n",
      "that - O\n",
      "' - O\n",
      "s - O\n",
      "Ce - B-CLEntity\n",
      "##les - I-CLEntity\n",
      "##tial - I-CLEntity\n",
      "Muse - L-CLEntity\n",
      "! - O\n",
      "and - O\n",
      "to - O\n",
      "our - O\n",
      "world - O\n",
      "relate - O\n",
      ". - O\n",
      "[SEP] - O\n"
     ]
    }
   ],
   "source": [
    "# try an exmaple\n",
    "from transformers import BertTokenizerFast, BatchEncoding\n",
    "from tokenizers import Encoding\n",
    "from alignment import align_tokens_and_annotations_bilou\n",
    "\n",
    "example = {'content': sequence, 'annotations': ex_annotations}\n",
    "tokenizer_ex = BertTokenizerFast.from_pretrained('bert-base-cased') # Load a pre-trained tokenizer\n",
    "tokenized_batch_ex : BatchEncoding = tokenizer_ex(example[\"content\"])\n",
    "tokenized_text : Encoding = tokenized_batch_ex[0]\n",
    "labels = align_tokens_and_annotations_bilou(tokenized_text, example[\"annotations\"])\n",
    "\n",
    "for token, label in zip(tokenized_text.tokens, labels):\n",
    "    print(token, \"-\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://huggingface.co/docs/tokenizers/python/latest/pipeline.html\n",
    "# tokenizer.decoder = decoders.WordPiece()\n",
    "# tokenizer.decode(outputs.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "odyssey_lines = []\n",
    "book = open(\"../example-texts/odyssey.txt\")\n",
    "for line in book:\n",
    "    line = line.strip()\n",
    "    odyssey_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "odyssey_lines = [line for line in odyssey_lines if line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The man for wisdom’s various arts renown’d,',\n",
       " 'Long exercised in woes, O Muse! resound;',\n",
       " 'Who, when his arms had wrought the destined fall',\n",
       " 'Of sacred Troy, and razed her heaven-built wall,',\n",
       " 'Wandering from clime to clime, observant stray’d,',\n",
       " 'Their manners noted, and their states survey’d,',\n",
       " 'On stormy seas unnumber’d toils he bore,',\n",
       " 'Safe with his friends to gain his natal shore:',\n",
       " 'Vain toils! their impious folly dared to prey',\n",
       " 'On herds devoted to the god of day;']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odyssey_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-238b170323ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordPiece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "source": [
    "## https://huggingface.co/docs/tokenizers/python/latest/pipeline.html\n",
    "# tokenizer.decoder = decoders.WordPiece()\n",
    "# tokenizer.decode(outputs.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'odyssey_lines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dc8970dbe8ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0modyssey_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtokens_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minputs_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutputs_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'odyssey_lines' is not defined"
     ]
    }
   ],
   "source": [
    "for line in odyssey_lines:\n",
    "    tokens_line = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(line)))\n",
    "    inputs_line = tokenizer.encode(line, return_tensors=\"pt\")\n",
    "\n",
    "    outputs_line = model(inputs_line).logits\n",
    "    predictions_line = torch.argmax(outputs_line, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = []\n",
    "book = open(\"../example-texts/odyssey.txt\")\n",
    "for line in book:\n",
    "    line = line.strip()\n",
    "    \n",
    "    line_data = dict()\n",
    "    line_data['content'] = line\n",
    "    line_data['annotations'] = get_annotations(line, pattern)\n",
    "    json_data.append(line_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': '', 'annotations': []},\n",
       " {'content': '', 'annotations': []},\n",
       " {'content': 'The man for wisdom’s various arts renown’d,', 'annotations': []},\n",
       " {'content': 'Long exercised in woes, O Muse! resound;',\n",
       "  'annotations': [{'start': 25, 'end': 30, 'label': 'CLEntity'}]},\n",
       " {'content': 'Who, when his arms had wrought the destined fall',\n",
       "  'annotations': []},\n",
       " {'content': 'Of sacred Troy, and razed her heaven-built wall,',\n",
       "  'annotations': [{'start': 9, 'end': 14, 'label': 'CLEntity'}]},\n",
       " {'content': 'Wandering from clime to clime, observant stray’d,',\n",
       "  'annotations': []},\n",
       " {'content': 'Their manners noted, and their states survey’d,',\n",
       "  'annotations': []},\n",
       " {'content': 'On stormy seas unnumber’d toils he bore,', 'annotations': []},\n",
       " {'content': 'Safe with his friends to gain his natal shore:',\n",
       "  'annotations': []}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For testing the result. Need a way to test whether tuned bert is accurately identifying CLEntities\n",
    "json_data[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda (base)",
   "language": "python",
   "name": "anaconda-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
