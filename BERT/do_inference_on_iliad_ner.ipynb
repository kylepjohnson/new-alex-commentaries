{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "from tokenizers import decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"bert_ner_finetuned_iliad-with-gpu.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['O', 'B-CLEntity', 'I-CLEntity', 'L-CLEntity', 'U-CLEntity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"\"\"The man for wisdom’s various arts renown’d,\n",
    "Long exercised in woes, O Muse! resound;\n",
    "Who, when his arms had wrought the destined fall\n",
    "Of sacred Troy, and razed her heaven-built wall,\n",
    "Wandering from clime to clime, observant stray’d,\n",
    "Their manners noted, and their states survey’d,\n",
    "On stormy seas unnumber’d toils he bore,\n",
    "Safe with his friends to gain his natal shore:\n",
    "Vain toils! their impious folly dared to prey\n",
    "On herds devoted to the god of day;\n",
    "The god vindictive doom’d them never more\n",
    "(Ah, men unbless’d!) to touch that natal shore.\n",
    "Oh, snatch some portion of these acts from fate,\n",
    "Celestial Muse! and to our world relate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "odyssey_lines = []\n",
    "book = open(\"../example-texts/odyssey.txt\")\n",
    "for line in book:\n",
    "    line = line.strip()\n",
    "    odyssey_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "odyssey_lines = [line for line in odyssey_lines if line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The man for wisdom’s various arts renown’d,',\n",
       " 'Long exercised in woes, O Muse! resound;',\n",
       " 'Who, when his arms had wrought the destined fall',\n",
       " 'Of sacred Troy, and razed her heaven-built wall,',\n",
       " 'Wandering from clime to clime, observant stray’d,',\n",
       " 'Their manners noted, and their states survey’d,',\n",
       " 'On stormy seas unnumber’d toils he bore,',\n",
       " 'Safe with his friends to gain his natal shore:',\n",
       " 'Vain toils! their impious folly dared to prey',\n",
       " 'On herds devoted to the god of day;']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odyssey_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
    "inputs = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(inputs).logits\n",
    "predictions = torch.argmax(outputs, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-238b170323ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordPiece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "source": [
    "## https://huggingface.co/docs/tokenizers/python/latest/pipeline.html\n",
    "# tokenizer.decoder = decoders.WordPiece()\n",
    "# tokenizer.decode(outputs.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 4, 0,\n",
       "         0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 2, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS]', 'LABEL_0')\n",
      "('The', 'LABEL_0')\n",
      "('man', 'LABEL_0')\n",
      "('for', 'LABEL_0')\n",
      "('wisdom', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('s', 'LABEL_0')\n",
      "('various', 'LABEL_0')\n",
      "('arts', 'LABEL_0')\n",
      "('re', 'LABEL_0')\n",
      "('##no', 'LABEL_0')\n",
      "('##wn', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('Long', 'LABEL_4')\n",
      "('exercised', 'LABEL_0')\n",
      "('in', 'LABEL_0')\n",
      "('w', 'LABEL_0')\n",
      "('##oes', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('O', 'LABEL_0')\n",
      "('Muse', 'LABEL_4')\n",
      "('!', 'LABEL_0')\n",
      "('re', 'LABEL_0')\n",
      "('##sound', 'LABEL_0')\n",
      "(';', 'LABEL_0')\n",
      "('Who', 'LABEL_4')\n",
      "(',', 'LABEL_0')\n",
      "('when', 'LABEL_0')\n",
      "('his', 'LABEL_0')\n",
      "('arms', 'LABEL_0')\n",
      "('had', 'LABEL_0')\n",
      "('wrought', 'LABEL_0')\n",
      "('the', 'LABEL_0')\n",
      "('destined', 'LABEL_0')\n",
      "('fall', 'LABEL_0')\n",
      "('Of', 'LABEL_0')\n",
      "('sacred', 'LABEL_0')\n",
      "('Troy', 'LABEL_4')\n",
      "(',', 'LABEL_0')\n",
      "('and', 'LABEL_0')\n",
      "('r', 'LABEL_0')\n",
      "('##azed', 'LABEL_0')\n",
      "('her', 'LABEL_0')\n",
      "('heaven', 'LABEL_0')\n",
      "('-', 'LABEL_0')\n",
      "('built', 'LABEL_0')\n",
      "('wall', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('Wan', 'LABEL_1')\n",
      "('##dering', 'LABEL_3')\n",
      "('from', 'LABEL_0')\n",
      "('c', 'LABEL_0')\n",
      "('##lim', 'LABEL_0')\n",
      "('##e', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('c', 'LABEL_0')\n",
      "('##lim', 'LABEL_0')\n",
      "('##e', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('o', 'LABEL_0')\n",
      "('##bs', 'LABEL_0')\n",
      "('##er', 'LABEL_0')\n",
      "('##vant', 'LABEL_0')\n",
      "('stray', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('Their', 'LABEL_4')\n",
      "('manners', 'LABEL_0')\n",
      "('noted', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('and', 'LABEL_0')\n",
      "('their', 'LABEL_0')\n",
      "('states', 'LABEL_0')\n",
      "('survey', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('On', 'LABEL_4')\n",
      "('storm', 'LABEL_0')\n",
      "('##y', 'LABEL_0')\n",
      "('seas', 'LABEL_0')\n",
      "('un', 'LABEL_0')\n",
      "('##num', 'LABEL_0')\n",
      "('##ber', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('##ils', 'LABEL_0')\n",
      "('he', 'LABEL_0')\n",
      "('bore', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('Safe', 'LABEL_4')\n",
      "('with', 'LABEL_0')\n",
      "('his', 'LABEL_0')\n",
      "('friends', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('gain', 'LABEL_0')\n",
      "('his', 'LABEL_0')\n",
      "('na', 'LABEL_0')\n",
      "('##tal', 'LABEL_0')\n",
      "('shore', 'LABEL_0')\n",
      "(':', 'LABEL_0')\n",
      "('V', 'LABEL_1')\n",
      "('##ain', 'LABEL_3')\n",
      "('to', 'LABEL_0')\n",
      "('##ils', 'LABEL_0')\n",
      "('!', 'LABEL_0')\n",
      "('their', 'LABEL_0')\n",
      "('imp', 'LABEL_0')\n",
      "('##ious', 'LABEL_0')\n",
      "('f', 'LABEL_0')\n",
      "('##olly', 'LABEL_0')\n",
      "('dared', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('prey', 'LABEL_0')\n",
      "('On', 'LABEL_0')\n",
      "('herd', 'LABEL_0')\n",
      "('##s', 'LABEL_0')\n",
      "('devoted', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('the', 'LABEL_0')\n",
      "('god', 'LABEL_0')\n",
      "('of', 'LABEL_0')\n",
      "('day', 'LABEL_0')\n",
      "(';', 'LABEL_0')\n",
      "('The', 'LABEL_4')\n",
      "('god', 'LABEL_0')\n",
      "('v', 'LABEL_0')\n",
      "('##ind', 'LABEL_0')\n",
      "('##ict', 'LABEL_0')\n",
      "('##ive', 'LABEL_0')\n",
      "('doom', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "('them', 'LABEL_0')\n",
      "('never', 'LABEL_0')\n",
      "('more', 'LABEL_0')\n",
      "('(', 'LABEL_0')\n",
      "('Ah', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('men', 'LABEL_0')\n",
      "('un', 'LABEL_0')\n",
      "('##bles', 'LABEL_0')\n",
      "('##s', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "('!', 'LABEL_0')\n",
      "(')', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('touch', 'LABEL_0')\n",
      "('that', 'LABEL_0')\n",
      "('na', 'LABEL_0')\n",
      "('##tal', 'LABEL_0')\n",
      "('shore', 'LABEL_0')\n",
      "('.', 'LABEL_0')\n",
      "('Oh', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('s', 'LABEL_0')\n",
      "('##natch', 'LABEL_0')\n",
      "('some', 'LABEL_0')\n",
      "('portion', 'LABEL_0')\n",
      "('of', 'LABEL_0')\n",
      "('these', 'LABEL_0')\n",
      "('acts', 'LABEL_0')\n",
      "('from', 'LABEL_0')\n",
      "('fate', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('Ce', 'LABEL_1')\n",
      "('##les', 'LABEL_2')\n",
      "('##tial', 'LABEL_3')\n",
      "('Muse', 'LABEL_4')\n",
      "('!', 'LABEL_0')\n",
      "('and', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('our', 'LABEL_0')\n",
      "('world', 'LABEL_0')\n",
      "('relate', 'LABEL_0')\n",
      "('.', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n"
     ]
    }
   ],
   "source": [
    "for token, prediction in zip(tokens, predictions[0].numpy()):\n",
    "     print((token, model.config.id2label[prediction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS]', 'LABEL_0')\n",
      "('The', 'LABEL_0')\n",
      "('man', 'LABEL_0')\n",
      "('for', 'LABEL_0')\n",
      "('wisdom', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('s', 'LABEL_0')\n",
      "('various', 'LABEL_0')\n",
      "('arts', 'LABEL_0')\n",
      "('re', 'LABEL_0')\n",
      "('##no', 'LABEL_0')\n",
      "('##wn', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('Long', 'LABEL_0')\n",
      "('exercised', 'LABEL_0')\n",
      "('in', 'LABEL_0')\n",
      "('w', 'LABEL_0')\n",
      "('##oes', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('O', 'LABEL_0')\n",
      "('Muse', 'LABEL_4')\n",
      "('!', 'LABEL_0')\n",
      "('re', 'LABEL_0')\n",
      "('##sound', 'LABEL_0')\n",
      "(';', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('Who', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('when', 'LABEL_0')\n",
      "('his', 'LABEL_0')\n",
      "('arms', 'LABEL_0')\n",
      "('had', 'LABEL_0')\n",
      "('wrought', 'LABEL_0')\n",
      "('the', 'LABEL_0')\n",
      "('destined', 'LABEL_0')\n",
      "('fall', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('Of', 'LABEL_0')\n",
      "('sacred', 'LABEL_0')\n",
      "('Troy', 'LABEL_4')\n",
      "(',', 'LABEL_0')\n",
      "('and', 'LABEL_0')\n",
      "('r', 'LABEL_0')\n",
      "('##azed', 'LABEL_0')\n",
      "('her', 'LABEL_0')\n",
      "('heaven', 'LABEL_0')\n",
      "('-', 'LABEL_0')\n",
      "('built', 'LABEL_0')\n",
      "('wall', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('Wan', 'LABEL_0')\n",
      "('##dering', 'LABEL_0')\n",
      "('from', 'LABEL_0')\n",
      "('c', 'LABEL_0')\n",
      "('##lim', 'LABEL_0')\n",
      "('##e', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('c', 'LABEL_0')\n",
      "('##lim', 'LABEL_0')\n",
      "('##e', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('o', 'LABEL_0')\n",
      "('##bs', 'LABEL_0')\n",
      "('##er', 'LABEL_0')\n",
      "('##vant', 'LABEL_0')\n",
      "('stray', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('Their', 'LABEL_0')\n",
      "('manners', 'LABEL_0')\n",
      "('noted', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('and', 'LABEL_0')\n",
      "('their', 'LABEL_0')\n",
      "('states', 'LABEL_0')\n",
      "('survey', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('On', 'LABEL_0')\n",
      "('storm', 'LABEL_0')\n",
      "('##y', 'LABEL_0')\n",
      "('seas', 'LABEL_0')\n",
      "('un', 'LABEL_0')\n",
      "('##num', 'LABEL_0')\n",
      "('##ber', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('##ils', 'LABEL_0')\n",
      "('he', 'LABEL_0')\n",
      "('bore', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('Safe', 'LABEL_0')\n",
      "('with', 'LABEL_0')\n",
      "('his', 'LABEL_0')\n",
      "('friends', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('gain', 'LABEL_0')\n",
      "('his', 'LABEL_0')\n",
      "('na', 'LABEL_0')\n",
      "('##tal', 'LABEL_0')\n",
      "('shore', 'LABEL_0')\n",
      "(':', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('V', 'LABEL_1')\n",
      "('##ain', 'LABEL_3')\n",
      "('to', 'LABEL_0')\n",
      "('##ils', 'LABEL_0')\n",
      "('!', 'LABEL_0')\n",
      "('their', 'LABEL_0')\n",
      "('imp', 'LABEL_0')\n",
      "('##ious', 'LABEL_0')\n",
      "('f', 'LABEL_0')\n",
      "('##olly', 'LABEL_0')\n",
      "('dared', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('prey', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('On', 'LABEL_0')\n",
      "('herd', 'LABEL_0')\n",
      "('##s', 'LABEL_0')\n",
      "('devoted', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('the', 'LABEL_0')\n",
      "('god', 'LABEL_0')\n",
      "('of', 'LABEL_0')\n",
      "('day', 'LABEL_0')\n",
      "(';', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('The', 'LABEL_0')\n",
      "('god', 'LABEL_0')\n",
      "('v', 'LABEL_0')\n",
      "('##ind', 'LABEL_0')\n",
      "('##ict', 'LABEL_0')\n",
      "('##ive', 'LABEL_0')\n",
      "('doom', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "('them', 'LABEL_0')\n",
      "('never', 'LABEL_0')\n",
      "('more', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('(', 'LABEL_0')\n",
      "('Ah', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('men', 'LABEL_0')\n",
      "('un', 'LABEL_0')\n",
      "('##bles', 'LABEL_0')\n",
      "('##s', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "('!', 'LABEL_0')\n",
      "(')', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('touch', 'LABEL_0')\n",
      "('that', 'LABEL_0')\n",
      "('na', 'LABEL_0')\n",
      "('##tal', 'LABEL_0')\n",
      "('shore', 'LABEL_0')\n",
      "('.', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('Oh', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('s', 'LABEL_0')\n",
      "('##natch', 'LABEL_0')\n",
      "('some', 'LABEL_0')\n",
      "('portion', 'LABEL_0')\n",
      "('of', 'LABEL_0')\n",
      "('these', 'LABEL_0')\n",
      "('acts', 'LABEL_0')\n",
      "('from', 'LABEL_0')\n",
      "('fate', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('Ce', 'LABEL_0')\n",
      "('##les', 'LABEL_0')\n",
      "('##tial', 'LABEL_0')\n",
      "('Muse', 'LABEL_4')\n",
      "('!', 'LABEL_0')\n",
      "('and', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('our', 'LABEL_0')\n",
      "('world', 'LABEL_0')\n",
      "('relate', 'LABEL_0')\n",
      "('.', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('Now', 'LABEL_0')\n",
      "('at', 'LABEL_0')\n",
      "('their', 'LABEL_0')\n",
      "('native', 'LABEL_0')\n",
      "('realm', 'LABEL_0')\n",
      "('##s', 'LABEL_0')\n",
      "('the', 'LABEL_0')\n",
      "('Greeks', 'LABEL_4')\n",
      "('arrived', 'LABEL_0')\n",
      "(';', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('All', 'LABEL_0')\n",
      "('who', 'LABEL_0')\n",
      "('the', 'LABEL_0')\n",
      "('wars', 'LABEL_0')\n",
      "('of', 'LABEL_0')\n",
      "('ten', 'LABEL_0')\n",
      "('long', 'LABEL_0')\n",
      "('years', 'LABEL_0')\n",
      "('survived', 'LABEL_0')\n",
      "(';', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('And', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('s', 'LABEL_0')\n",
      "('##cape', 'LABEL_0')\n",
      "('##d', 'LABEL_0')\n",
      "('the', 'LABEL_0')\n",
      "('per', 'LABEL_0')\n",
      "('##ils', 'LABEL_0')\n",
      "('of', 'LABEL_0')\n",
      "('the', 'LABEL_0')\n",
      "('g', 'LABEL_0')\n",
      "('##ulf', 'LABEL_0')\n",
      "('##y', 'LABEL_0')\n",
      "('main', 'LABEL_0')\n",
      "('.', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('Ulysses', 'LABEL_4')\n",
      "(',', 'LABEL_0')\n",
      "('sole', 'LABEL_0')\n",
      "('of', 'LABEL_0')\n",
      "('all', 'LABEL_0')\n",
      "('the', 'LABEL_0')\n",
      "('v', 'LABEL_0')\n",
      "('##ict', 'LABEL_0')\n",
      "('##or', 'LABEL_0')\n",
      "('train', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('An', 'LABEL_0')\n",
      "('exile', 'LABEL_0')\n",
      "('from', 'LABEL_0')\n",
      "('his', 'LABEL_0')\n",
      "('dear', 'LABEL_0')\n",
      "('paternal', 'LABEL_0')\n",
      "('coast', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('De', 'LABEL_0')\n",
      "('##p', 'LABEL_0')\n",
      "('##lore', 'LABEL_0')\n",
      "('##d', 'LABEL_0')\n",
      "('his', 'LABEL_0')\n",
      "('absent', 'LABEL_0')\n",
      "('queen', 'LABEL_0')\n",
      "('and', 'LABEL_0')\n",
      "('empire', 'LABEL_0')\n",
      "('lost', 'LABEL_0')\n",
      "('.', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('Cal', 'LABEL_1')\n",
      "('##y', 'LABEL_2')\n",
      "('##ps', 'LABEL_2')\n",
      "('##o', 'LABEL_3')\n",
      "('in', 'LABEL_0')\n",
      "('her', 'LABEL_0')\n",
      "('caves', 'LABEL_0')\n",
      "('con', 'LABEL_0')\n",
      "('##stra', 'LABEL_0')\n",
      "('##in', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "('his', 'LABEL_0')\n",
      "('stay', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('With', 'LABEL_0')\n",
      "('sweet', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('reluctant', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('am', 'LABEL_0')\n",
      "('##orous', 'LABEL_0')\n",
      "('delay', 'LABEL_0')\n",
      "(';', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('In', 'LABEL_0')\n",
      "('vain', 'LABEL_0')\n",
      "('-', 'LABEL_0')\n",
      "('for', 'LABEL_0')\n",
      "('now', 'LABEL_0')\n",
      "('the', 'LABEL_0')\n",
      "('circling', 'LABEL_0')\n",
      "('years', 'LABEL_0')\n",
      "('disc', 'LABEL_0')\n",
      "('##lose', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('The', 'LABEL_0')\n",
      "('day', 'LABEL_0')\n",
      "('pre', 'LABEL_0')\n",
      "('##destine', 'LABEL_0')\n",
      "('##d', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('reward', 'LABEL_0')\n",
      "('his', 'LABEL_0')\n",
      "('w', 'LABEL_0')\n",
      "('##oes', 'LABEL_0')\n",
      "('.', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('At', 'LABEL_0')\n",
      "('length', 'LABEL_0')\n",
      "('his', 'LABEL_0')\n",
      "('It', 'LABEL_1')\n",
      "('##ha', 'LABEL_2')\n",
      "('##ca', 'LABEL_3')\n",
      "('is', 'LABEL_0')\n",
      "('given', 'LABEL_0')\n",
      "('by', 'LABEL_0')\n",
      "('fate', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('Where', 'LABEL_0')\n",
      "('yet', 'LABEL_0')\n",
      "('new', 'LABEL_0')\n",
      "('labour', 'LABEL_0')\n",
      "('##s', 'LABEL_0')\n",
      "('his', 'LABEL_0')\n",
      "('arrival', 'LABEL_0')\n",
      "('wait', 'LABEL_0')\n",
      "(';', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('At', 'LABEL_0')\n",
      "('length', 'LABEL_0')\n",
      "('their', 'LABEL_0')\n",
      "('rage', 'LABEL_0')\n",
      "('the', 'LABEL_0')\n",
      "('hostile', 'LABEL_0')\n",
      "('powers', 'LABEL_0')\n",
      "('rest', 'LABEL_0')\n",
      "('##rain', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('All', 'LABEL_0')\n",
      "('but', 'LABEL_0')\n",
      "('the', 'LABEL_0')\n",
      "('ruthless', 'LABEL_0')\n",
      "('monarch', 'LABEL_0')\n",
      "('of', 'LABEL_0')\n",
      "('the', 'LABEL_0')\n",
      "('main', 'LABEL_0')\n",
      "('.', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('But', 'LABEL_0')\n",
      "('now', 'LABEL_0')\n",
      "('the', 'LABEL_0')\n",
      "('god', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('remote', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('a', 'LABEL_0')\n",
      "('heavenly', 'LABEL_0')\n",
      "('guest', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('In', 'LABEL_0')\n",
      "('Æ', 'LABEL_0')\n",
      "('##thi', 'LABEL_0')\n",
      "('##op', 'LABEL_0')\n",
      "('##ia', 'LABEL_0')\n",
      "('grace', 'LABEL_0')\n",
      "('##d', 'LABEL_0')\n",
      "('the', 'LABEL_0')\n",
      "('g', 'LABEL_0')\n",
      "('##enia', 'LABEL_0')\n",
      "('##l', 'LABEL_0')\n",
      "('feast', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n",
      "('[CLS]', 'LABEL_0')\n",
      "('(', 'LABEL_0')\n",
      "('A', 'LABEL_0')\n",
      "('race', 'LABEL_0')\n",
      "('divided', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('whom', 'LABEL_0')\n",
      "('with', 'LABEL_0')\n",
      "('s', 'LABEL_0')\n",
      "('##loping', 'LABEL_0')\n",
      "('rays', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS]', 'LABEL_0')\n",
      "('The', 'LABEL_0')\n",
      "('rising', 'LABEL_0')\n",
      "('and', 'LABEL_0')\n",
      "('descending', 'LABEL_0')\n",
      "('sun', 'LABEL_0')\n",
      "('surveys', 'LABEL_0')\n",
      "(')', 'LABEL_0')\n",
      "(';', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-46047abe8df6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minputs_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0moutputs_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mpredictions_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_line\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1713\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    989\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         )\n\u001b[0;32m--> 991\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    992\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 )\n\u001b[1;32m    581\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    583\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         )\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   1999\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2001\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for line in odyssey_lines:\n",
    "    tokens_line = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(line)))\n",
    "    inputs_line = tokenizer.encode(line, return_tensors=\"pt\")\n",
    "\n",
    "    outputs_line = model(inputs_line).logits\n",
    "    predictions_line = torch.argmax(outputs_line, dim=2)\n",
    "    for token, prediction in zip(tokens_line, predictions_line[0].numpy()):\n",
    "         print((token, model.config.id2label[prediction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r' ([A-Z].[a-z]+)'\n",
    "re.compile(pattern)\n",
    "\n",
    "def get_annotations(text, pattern):\n",
    "    annotations = []\n",
    "    for match in re.finditer(pattern, text):\n",
    "        label_dic = dict()\n",
    "        label_dic['start'] = match.start()\n",
    "        label_dic['end'] = match.end()\n",
    "        label_dic['label'] = 'CLEntity' # Entity starting with a capital letter\n",
    "        annotations.append(label_dic)\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = []\n",
    "book = open(\"../example-texts/odyssey.txt\")\n",
    "for line in book:\n",
    "    line = line.strip()\n",
    "    \n",
    "    line_data = dict()\n",
    "    line_data['content'] = line\n",
    "    line_data['annotations'] = get_annotations(line, pattern)\n",
    "    json_data.append(line_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': '', 'annotations': []},\n",
       " {'content': '', 'annotations': []},\n",
       " {'content': 'The man for wisdom’s various arts renown’d,', 'annotations': []},\n",
       " {'content': 'Long exercised in woes, O Muse! resound;',\n",
       "  'annotations': [{'start': 25, 'end': 30, 'label': 'CLEntity'}]},\n",
       " {'content': 'Who, when his arms had wrought the destined fall',\n",
       "  'annotations': []},\n",
       " {'content': 'Of sacred Troy, and razed her heaven-built wall,',\n",
       "  'annotations': [{'start': 9, 'end': 14, 'label': 'CLEntity'}]},\n",
       " {'content': 'Wandering from clime to clime, observant stray’d,',\n",
       "  'annotations': []},\n",
       " {'content': 'Their manners noted, and their states survey’d,',\n",
       "  'annotations': []},\n",
       " {'content': 'On stormy seas unnumber’d toils he bore,', 'annotations': []},\n",
       " {'content': 'Safe with his friends to gain his natal shore:',\n",
       "  'annotations': []}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For testing the result. Need a way to test whether tuned bert is accurately identifying CLEntities\n",
    "json_data[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda (base)",
   "language": "python",
   "name": "anaconda-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
