{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch\n",
    "from transformers import BertForTokenClassification, BertTokenizer, BertTokenizerFast\n",
    "\n",
    "from tokenizers import decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"bert_ner_finetuned_iliad-with-gpu-pattern2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['O', 'B-CLEntity', 'I-CLEntity', 'L-CLEntity', 'U-CLEntity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"\"\"The man for wisdom’s various arts renown’d,\n",
    "Long exercised in woes, O Muse! resound;\n",
    "Who, when his arms had wrought the destined fall\n",
    "Of sacred Troy, and razed her heaven-built wall,\n",
    "Wandering from clime to clime, observant stray’d,\n",
    "Their manners noted, and their states survey’d,\n",
    "On stormy seas unnumber’d toils he bore,\n",
    "Safe with his friends to gain his natal shore:\n",
    "Vain toils! their impious folly dared to prey\n",
    "On herds devoted to the god of day;\n",
    "The god vindictive doom’d them never more\n",
    "(Ah, men unbless’d!) to touch that natal shore.\n",
    "Oh, snatch some portion of these acts from fate,\n",
    "that's Celestial Muse! and to our world relate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'The', 'man', 'for', 'wisdom', '’', 's', 'various', 'arts', 're']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(sequence, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1109,  1299,  1111, 12304,   787,   188,  1672,  3959,  1231])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.9492, -2.2200, -2.9792, -2.1944, -2.7990],\n",
       "        [-1.0060,  0.3633, -3.5774, -1.9558,  6.7369],\n",
       "        [ 8.5389, -2.9141, -3.1272, -1.7843, -2.3615],\n",
       "        [ 9.0322, -2.8466, -2.8821, -2.0914, -2.9410],\n",
       "        [ 9.0151, -2.5689, -2.9633, -2.2023, -2.8106],\n",
       "        [ 9.2213, -2.5399, -2.8564, -2.3591, -2.9243],\n",
       "        [ 9.4320, -2.7111, -2.9689, -2.3696, -3.0193],\n",
       "        [ 9.2810, -2.6317, -2.8026, -2.3994, -2.9145],\n",
       "        [ 9.2640, -2.6496, -2.7412, -2.2737, -2.9194],\n",
       "        [ 9.3333, -2.7478, -2.7948, -2.3169, -3.1223]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.argmax(outputs, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS]', 'LABEL_0')\n",
      "('The', 'LABEL_4')\n",
      "('man', 'LABEL_0')\n",
      "('for', 'LABEL_0')\n",
      "('wisdom', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('s', 'LABEL_0')\n",
      "('various', 'LABEL_0')\n",
      "('arts', 'LABEL_0')\n",
      "('re', 'LABEL_0')\n",
      "('##no', 'LABEL_0')\n",
      "('##wn', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('Long', 'LABEL_4')\n",
      "('exercised', 'LABEL_0')\n",
      "('in', 'LABEL_0')\n",
      "('w', 'LABEL_0')\n",
      "('##oes', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('O', 'LABEL_0')\n",
      "('Muse', 'LABEL_4')\n",
      "('!', 'LABEL_0')\n",
      "('re', 'LABEL_0')\n",
      "('##sound', 'LABEL_0')\n",
      "(';', 'LABEL_0')\n",
      "('Who', 'LABEL_4')\n",
      "(',', 'LABEL_0')\n",
      "('when', 'LABEL_0')\n",
      "('his', 'LABEL_0')\n",
      "('arms', 'LABEL_0')\n",
      "('had', 'LABEL_0')\n",
      "('wrought', 'LABEL_0')\n",
      "('the', 'LABEL_0')\n",
      "('destined', 'LABEL_0')\n",
      "('fall', 'LABEL_0')\n",
      "('Of', 'LABEL_4')\n",
      "('sacred', 'LABEL_0')\n",
      "('Troy', 'LABEL_4')\n",
      "(',', 'LABEL_0')\n",
      "('and', 'LABEL_0')\n",
      "('r', 'LABEL_0')\n",
      "('##azed', 'LABEL_0')\n",
      "('her', 'LABEL_0')\n",
      "('heaven', 'LABEL_0')\n",
      "('-', 'LABEL_0')\n",
      "('built', 'LABEL_0')\n",
      "('wall', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('Wan', 'LABEL_1')\n",
      "('##dering', 'LABEL_3')\n",
      "('from', 'LABEL_0')\n",
      "('c', 'LABEL_0')\n",
      "('##lim', 'LABEL_0')\n",
      "('##e', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('c', 'LABEL_0')\n",
      "('##lim', 'LABEL_0')\n",
      "('##e', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('o', 'LABEL_0')\n",
      "('##bs', 'LABEL_0')\n",
      "('##er', 'LABEL_0')\n",
      "('##vant', 'LABEL_0')\n",
      "('stray', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('Their', 'LABEL_4')\n",
      "('manners', 'LABEL_0')\n",
      "('noted', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('and', 'LABEL_0')\n",
      "('their', 'LABEL_0')\n",
      "('states', 'LABEL_0')\n",
      "('survey', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('On', 'LABEL_4')\n",
      "('storm', 'LABEL_0')\n",
      "('##y', 'LABEL_0')\n",
      "('seas', 'LABEL_0')\n",
      "('un', 'LABEL_0')\n",
      "('##num', 'LABEL_0')\n",
      "('##ber', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('##ils', 'LABEL_0')\n",
      "('he', 'LABEL_0')\n",
      "('bore', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('Safe', 'LABEL_4')\n",
      "('with', 'LABEL_0')\n",
      "('his', 'LABEL_0')\n",
      "('friends', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('gain', 'LABEL_0')\n",
      "('his', 'LABEL_0')\n",
      "('na', 'LABEL_0')\n",
      "('##tal', 'LABEL_0')\n",
      "('shore', 'LABEL_0')\n",
      "(':', 'LABEL_0')\n",
      "('V', 'LABEL_1')\n",
      "('##ain', 'LABEL_3')\n",
      "('to', 'LABEL_0')\n",
      "('##ils', 'LABEL_0')\n",
      "('!', 'LABEL_0')\n",
      "('their', 'LABEL_0')\n",
      "('imp', 'LABEL_0')\n",
      "('##ious', 'LABEL_0')\n",
      "('f', 'LABEL_0')\n",
      "('##olly', 'LABEL_0')\n",
      "('dared', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('prey', 'LABEL_0')\n",
      "('On', 'LABEL_4')\n",
      "('herd', 'LABEL_0')\n",
      "('##s', 'LABEL_0')\n",
      "('devoted', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('the', 'LABEL_0')\n",
      "('god', 'LABEL_0')\n",
      "('of', 'LABEL_0')\n",
      "('day', 'LABEL_0')\n",
      "(';', 'LABEL_0')\n",
      "('The', 'LABEL_4')\n",
      "('god', 'LABEL_0')\n",
      "('v', 'LABEL_0')\n",
      "('##ind', 'LABEL_0')\n",
      "('##ict', 'LABEL_0')\n",
      "('##ive', 'LABEL_0')\n",
      "('doom', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "('them', 'LABEL_0')\n",
      "('never', 'LABEL_0')\n",
      "('more', 'LABEL_0')\n",
      "('(', 'LABEL_0')\n",
      "('Ah', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('men', 'LABEL_0')\n",
      "('un', 'LABEL_0')\n",
      "('##bles', 'LABEL_0')\n",
      "('##s', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "('!', 'LABEL_0')\n",
      "(')', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('touch', 'LABEL_0')\n",
      "('that', 'LABEL_0')\n",
      "('na', 'LABEL_0')\n",
      "('##tal', 'LABEL_0')\n",
      "('shore', 'LABEL_0')\n",
      "('.', 'LABEL_0')\n",
      "('Oh', 'LABEL_4')\n",
      "(',', 'LABEL_0')\n",
      "('s', 'LABEL_0')\n",
      "('##natch', 'LABEL_0')\n",
      "('some', 'LABEL_0')\n",
      "('portion', 'LABEL_0')\n",
      "('of', 'LABEL_0')\n",
      "('these', 'LABEL_0')\n",
      "('acts', 'LABEL_0')\n",
      "('from', 'LABEL_0')\n",
      "('fate', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('that', 'LABEL_0')\n",
      "(\"'\", 'LABEL_0')\n",
      "('s', 'LABEL_0')\n",
      "('Ce', 'LABEL_1')\n",
      "('##les', 'LABEL_2')\n",
      "('##tial', 'LABEL_3')\n",
      "('Muse', 'LABEL_4')\n",
      "('!', 'LABEL_0')\n",
      "('and', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('our', 'LABEL_0')\n",
      "('world', 'LABEL_0')\n",
      "('relate', 'LABEL_0')\n",
      "('.', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n"
     ]
    }
   ],
   "source": [
    "for token, prediction in zip(tokens, predictions[0].numpy()):\n",
    "     print((token, model.config.id2label[prediction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LABEL_0', 'LABEL_4', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_4', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_4', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_4', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_4', 'LABEL_0', 'LABEL_4', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_1', 'LABEL_3', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_4', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_4', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_4', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_1', 'LABEL_3', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_4', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_4', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_4', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_1', 'LABEL_2', 'LABEL_3', 'LABEL_4', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0']\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "pred_line_label = []\n",
    "for prediction in predictions[0].numpy():\n",
    "    pred_line_label.append(model.config.id2label[prediction])\n",
    "pred.append(pred_line_label)\n",
    "print(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern = r' ([A-Z].[a-z]+)'\n",
    "pattern = r'(\\b[A-Z][a-z]+\\b)(\\s\\b[A-Z][a-z]+\\b)*'\n",
    "re.compile(pattern)\n",
    "\n",
    "def get_annotations(text, pattern):\n",
    "    annotations = []\n",
    "    for match in re.finditer(pattern, text):\n",
    "        label_dic = dict()\n",
    "        label_dic['start'] = match.start()\n",
    "        label_dic['end'] = match.end()\n",
    "        label_dic['label'] = 'CLEntity' # Entity starting with a capital letter\n",
    "        annotations.append(label_dic)\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_annotations = get_annotations(sequence, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try an exmaple\n",
    "from transformers import BertTokenizerFast, BatchEncoding\n",
    "from tokenizers import Encoding\n",
    "from alignment import align_tokens_and_annotations_bilou\n",
    "\n",
    "example = {'content': sequence, 'annotations': ex_annotations}\n",
    "tokenizer_ex = BertTokenizerFast.from_pretrained('bert-base-cased') # Load a pre-trained tokenizer\n",
    "tokenized_batch_ex : BatchEncoding = tokenizer_ex(example[\"content\"])\n",
    "tokenized_text : Encoding = tokenized_batch_ex[0]\n",
    "labels = align_tokens_and_annotations_bilou(tokenized_text, example[\"annotations\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] - 0\n",
      "The - 4\n",
      "man - 0\n",
      "for - 0\n",
      "wisdom - 0\n",
      "’ - 0\n",
      "s - 0\n",
      "various - 0\n",
      "arts - 0\n",
      "re - 0\n",
      "##no - 0\n",
      "##wn - 0\n",
      "’ - 0\n",
      "d - 0\n",
      ", - 0\n",
      "Long - 4\n",
      "exercised - 0\n",
      "in - 0\n",
      "w - 0\n",
      "##oes - 0\n",
      ", - 0\n",
      "O - 0\n",
      "Muse - 4\n",
      "! - 0\n",
      "re - 0\n",
      "##sound - 0\n",
      "; - 0\n",
      "Who - 4\n",
      ", - 0\n",
      "when - 0\n",
      "his - 0\n",
      "arms - 0\n",
      "had - 0\n",
      "wrought - 0\n",
      "the - 0\n",
      "destined - 0\n",
      "fall - 0\n",
      "Of - 4\n",
      "sacred - 0\n",
      "Troy - 4\n",
      ", - 0\n",
      "and - 0\n",
      "r - 0\n",
      "##azed - 0\n",
      "her - 0\n",
      "heaven - 0\n",
      "- - 0\n",
      "built - 0\n",
      "wall - 0\n",
      ", - 0\n",
      "Wan - 1\n",
      "##dering - 3\n",
      "from - 0\n",
      "c - 0\n",
      "##lim - 0\n",
      "##e - 0\n",
      "to - 0\n",
      "c - 0\n",
      "##lim - 0\n",
      "##e - 0\n",
      ", - 0\n",
      "o - 0\n",
      "##bs - 0\n",
      "##er - 0\n",
      "##vant - 0\n",
      "stray - 0\n",
      "’ - 0\n",
      "d - 0\n",
      ", - 0\n",
      "Their - 4\n",
      "manners - 0\n",
      "noted - 0\n",
      ", - 0\n",
      "and - 0\n",
      "their - 0\n",
      "states - 0\n",
      "survey - 0\n",
      "’ - 0\n",
      "d - 0\n",
      ", - 0\n",
      "On - 4\n",
      "storm - 0\n",
      "##y - 0\n",
      "seas - 0\n",
      "un - 0\n",
      "##num - 0\n",
      "##ber - 0\n",
      "’ - 0\n",
      "d - 0\n",
      "to - 0\n",
      "##ils - 0\n",
      "he - 0\n",
      "bore - 0\n",
      ", - 0\n",
      "Safe - 4\n",
      "with - 0\n",
      "his - 0\n",
      "friends - 0\n",
      "to - 0\n",
      "gain - 0\n",
      "his - 0\n",
      "na - 0\n",
      "##tal - 0\n",
      "shore - 0\n",
      ": - 0\n",
      "V - 1\n",
      "##ain - 3\n",
      "to - 0\n",
      "##ils - 0\n",
      "! - 0\n",
      "their - 0\n",
      "imp - 0\n",
      "##ious - 0\n",
      "f - 0\n",
      "##olly - 0\n",
      "dared - 0\n",
      "to - 0\n",
      "prey - 0\n",
      "On - 4\n",
      "herd - 0\n",
      "##s - 0\n",
      "devoted - 0\n",
      "to - 0\n",
      "the - 0\n",
      "god - 0\n",
      "of - 0\n",
      "day - 0\n",
      "; - 0\n",
      "The - 4\n",
      "god - 0\n",
      "v - 0\n",
      "##ind - 0\n",
      "##ict - 0\n",
      "##ive - 0\n",
      "doom - 0\n",
      "’ - 0\n",
      "d - 0\n",
      "them - 0\n",
      "never - 0\n",
      "more - 0\n",
      "( - 0\n",
      "Ah - 4\n",
      ", - 0\n",
      "men - 0\n",
      "un - 0\n",
      "##bles - 0\n",
      "##s - 0\n",
      "’ - 0\n",
      "d - 0\n",
      "! - 0\n",
      ") - 0\n",
      "to - 0\n",
      "touch - 0\n",
      "that - 0\n",
      "na - 0\n",
      "##tal - 0\n",
      "shore - 0\n",
      ". - 0\n",
      "Oh - 4\n",
      ", - 0\n",
      "s - 0\n",
      "##natch - 0\n",
      "some - 0\n",
      "portion - 0\n",
      "of - 0\n",
      "these - 0\n",
      "acts - 0\n",
      "from - 0\n",
      "fate - 0\n",
      ", - 0\n",
      "that - 0\n",
      "' - 0\n",
      "s - 0\n",
      "Ce - 1\n",
      "##les - 2\n",
      "##tial - 2\n",
      "Muse - 3\n",
      "! - 0\n",
      "and - 0\n",
      "to - 0\n",
      "our - 0\n",
      "world - 0\n",
      "relate - 0\n",
      ". - 0\n",
      "[SEP] - 0\n"
     ]
    }
   ],
   "source": [
    "from labelset import LabelSet\n",
    "\n",
    "ex_label_set = LabelSet(labels=[\"CLEntity\"])\n",
    "aligned_label_ids = ex_label_set.get_aligned_label_ids_from_annotations(\n",
    "    tokenized_text, example[\"annotations\"]\n",
    ")\n",
    "tokens = tokenized_text.tokens\n",
    "\n",
    "for token, label in zip(tokens, aligned_label_ids):\n",
    "    print(token, \"-\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LABEL_0', 'LABEL_4', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0']\n"
     ]
    }
   ],
   "source": [
    "true = []\n",
    "true_line_label = []\n",
    "for label in aligned_label_ids:\n",
    "    true_line_label.append('LABEL_'+str(label))\n",
    "true.append(true_line_label)\n",
    "print(true[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seqeval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-a40ac80f6312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mseqeval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mseqeval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mseqeval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seqeval'"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "classification_report(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://huggingface.co/docs/tokenizers/python/latest/pipeline.html\n",
    "# tokenizer.decoder = decoders.WordPiece()\n",
    "# tokenizer.decode(outputs.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odyssey_lines = []\n",
    "book = open(\"../example-texts/odyssey.txt\")\n",
    "for line in book:\n",
    "    line = line.strip()\n",
    "    odyssey_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odyssey_lines = [line for line in odyssey_lines if line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odyssey_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## https://huggingface.co/docs/tokenizers/python/latest/pipeline.html\n",
    "# tokenizer.decoder = decoders.WordPiece()\n",
    "# tokenizer.decode(outputs.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in odyssey_lines:\n",
    "    tokens_line = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(line)))\n",
    "    inputs_line = tokenizer.encode(line, return_tensors=\"pt\")\n",
    "\n",
    "    outputs_line = model(inputs_line).logits\n",
    "    predictions_line = torch.argmax(outputs_line, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = []\n",
    "book = open(\"../example-texts/odyssey.txt\")\n",
    "for line in book:\n",
    "    line = line.strip()\n",
    "    \n",
    "    line_data = dict()\n",
    "    line_data['content'] = line\n",
    "    line_data['annotations'] = get_annotations(line, pattern)\n",
    "    json_data.append(line_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing the result. Need a way to test whether tuned bert is accurately identifying CLEntities\n",
    "json_data[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda (base)",
   "language": "python",
   "name": "anaconda-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
