{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List,Any\n",
    "IntList = List[int] # A list of token_ids\n",
    "IntListList = List[IntList] # A List of List of token_ids, e.g. a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 103, 'end': 114, 'text': 'Archilochus', 'label': 'CL-Entity'}, {'start': 116, 'end': 125, 'text': 'Simonides', 'label': 'CL-Entity'}, {'start': 129, 'end': 136, 'text': 'Amorgus', 'label': 'CL-Entity'}, {'start': 138, 'end': 146, 'text': 'Kallinus', 'label': 'CL-Entity'}, {'start': 148, 'end': 156, 'text': 'Tyrtaeus', 'label': 'CL-Entity'}, {'start': 158, 'end': 165, 'text': 'Xanthus', 'label': 'CL-Entity'}]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = r'[A-Z][A-Za-z]*'\n",
    "re.compile(pattern)\n",
    "\n",
    "text = \"\"\"and the early\n",
    "inscriptions are rude and unskilfully executed; nor can we even assure\n",
    "ourselves whether Archilochus, Simonides of Amorgus, Kallinus,\n",
    "Tyrtaeus, Xanthus, and the other early elegiac and lyric poets,\n",
    "committed their compositions to writing, or at what time the practice\n",
    "of doing so became familiar.\"\"\"\n",
    "\n",
    "annotations = []\n",
    "for match in re.finditer(pattern, text):\n",
    "    label_dic = dict()\n",
    "    label_dic['start'] = match.start()\n",
    "    label_dic['end'] = match.end()\n",
    "    label_dic['text'] = text[match.start():match.end()]\n",
    "    label_dic['label'] = 'CL-Entity' # Entity starting with a capital letter\n",
    "    annotations.append(label_dic)\n",
    "print(annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_tokens_and_annotations_bilou(tokenized: Encoding, annotations):\n",
    "    tokens = tokenized.tokens\n",
    "    aligned_labels = [\"O\"] * len(\n",
    "        tokens\n",
    "    )  # Make a list to store our labels the same length as our tokens\n",
    "    for anno in annotations:\n",
    "        annotation_token_ix_set = (\n",
    "            set()\n",
    "        )  # A set that stores the token indices of the annotation\n",
    "        for char_ix in range(anno[\"start\"], anno[\"end\"]):\n",
    "\n",
    "            token_ix = tokenized.char_to_token(char_ix)\n",
    "            if token_ix is not None:\n",
    "                annotation_token_ix_set.add(token_ix)\n",
    "        if len(annotation_token_ix_set) == 1:\n",
    "            # If there is only one token\n",
    "            token_ix = annotation_token_ix_set.pop()\n",
    "            prefix = (\n",
    "                \"U\"  # This annotation spans one token so is prefixed with U for unique\n",
    "            )\n",
    "            aligned_labels[token_ix] = f\"{prefix}-{anno['label']}\"\n",
    "\n",
    "        else:\n",
    "\n",
    "            last_token_in_anno_ix = len(annotation_token_ix_set) - 1\n",
    "            for num, token_ix in enumerate(sorted(annotation_token_ix_set)):\n",
    "                if num == 0:\n",
    "                    prefix = \"B\"\n",
    "                elif num == last_token_in_anno_ix:\n",
    "                    prefix = \"L\"  # Its the last token\n",
    "                else:\n",
    "                    prefix = \"I\"  # We're inside of a multi token annotation\n",
    "                aligned_labels[token_ix] = f\"{prefix}-{anno['label']}\"\n",
    "    return aligned_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] - O\n",
      "and - O\n",
      "the - O\n",
      "early - O\n",
      "inscriptions - O\n",
      "are - O\n",
      "rude - O\n",
      "and - O\n",
      "un - O\n",
      "##ski - O\n",
      "##lf - O\n",
      "##ully - O\n",
      "executed - O\n",
      "; - O\n",
      "nor - O\n",
      "can - O\n",
      "we - O\n",
      "even - O\n",
      "assure - O\n",
      "ourselves - O\n",
      "whether - O\n",
      "Arch - B-CL-Entity\n",
      "##ilo - I-CL-Entity\n",
      "##chus - L-CL-Entity\n",
      ", - O\n",
      "Simon - B-CL-Entity\n",
      "##ides - L-CL-Entity\n",
      "of - O\n",
      "Amor - B-CL-Entity\n",
      "##gus - L-CL-Entity\n",
      ", - O\n",
      "Ka - B-CL-Entity\n",
      "##llin - I-CL-Entity\n",
      "##us - L-CL-Entity\n",
      ", - O\n",
      "Ty - B-CL-Entity\n",
      "##rta - I-CL-Entity\n",
      "##eus - L-CL-Entity\n",
      ", - O\n",
      "X - B-CL-Entity\n",
      "##ant - I-CL-Entity\n",
      "##hus - L-CL-Entity\n",
      ", - O\n",
      "and - O\n",
      "the - O\n",
      "other - O\n",
      "early - O\n",
      "el - O\n",
      "##eg - O\n",
      "##iac - O\n",
      "and - O\n",
      "lyric - O\n",
      "poets - O\n",
      ", - O\n",
      "committed - O\n",
      "their - O\n",
      "compositions - O\n",
      "to - O\n",
      "writing - O\n",
      ", - O\n",
      "or - O\n",
      "at - O\n",
      "what - O\n",
      "time - O\n",
      "the - O\n",
      "practice - O\n",
      "of - O\n",
      "doing - O\n",
      "so - O\n",
      "became - O\n",
      "familiar - O\n",
      ". - O\n",
      "[SEP] - O\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast,  BatchEncoding\n",
    "from tokenizers import Encoding\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased') # Load a pre-trained tokenizer\n",
    "tokenized_batch : BatchEncoding = tokenizer(text)\n",
    "tokenized_text : Encoding = tokenized_batch[0]\n",
    "labels = align_tokens_and_annotations_bilou(tokenized_text, annotations)\n",
    "for token, label in zip(tokenized_text.tokens, labels):\n",
    "    print(token, \"-\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda (base)",
   "language": "python",
   "name": "anaconda-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
