{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch\n",
    "from transformers import BertForTokenClassification, BertTokenizer, BertTokenizerFast\n",
    "from transformers import pipeline\n",
    "from tokenizers import decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"bert_ner_finetuned_iliad-with-gpu-pattern2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['O', 'B-CLEntity', 'I-CLEntity', 'L-CLEntity', 'U-CLEntity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"\"\"The man for wisdom’s various arts renown’d,\n",
    "Long exercised in woes, O Muse! resound;\n",
    "Who, when his arms had wrought the destined fall\n",
    "Of sacred Troy, and razed her heaven-built wall,\n",
    "Wandering from clime to clime, observant stray’d,\n",
    "Their manners noted, and their states survey’d,\n",
    "On stormy seas unnumber’d toils he bore,\n",
    "Safe with his friends to gain his natal shore:\n",
    "Vain toils! their impious folly dared to prey\n",
    "On herds devoted to the god of day;\n",
    "The god vindictive doom’d them never more\n",
    "(Ah, men unbless’d!) to touch that natal shore.\n",
    "Oh, snatch some portion of these acts from fate,\n",
    "that's Celestial Muse! and to our world relate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'LABEL_4', 'score': 0.80995053, 'index': 1, 'word': 'The', 'start': 0, 'end': 3}, {'entity': 'LABEL_0', 'score': 0.9999246, 'index': 2, 'word': 'man', 'start': 4, 'end': 7}, {'entity': 'LABEL_0', 'score': 0.9999582, 'index': 3, 'word': 'for', 'start': 8, 'end': 11}, {'entity': 'LABEL_0', 'score': 0.9999104, 'index': 4, 'word': 'wisdom', 'start': 12, 'end': 18}, {'entity': 'LABEL_0', 'score': 0.99995613, 'index': 5, 'word': '’', 'start': 18, 'end': 19}, {'entity': 'LABEL_0', 'score': 0.999963, 'index': 6, 'word': 's', 'start': 19, 'end': 20}, {'entity': 'LABEL_0', 'score': 0.99996036, 'index': 7, 'word': 'various', 'start': 21, 'end': 28}, {'entity': 'LABEL_0', 'score': 0.99993396, 'index': 8, 'word': 'arts', 'start': 29, 'end': 33}, {'entity': 'LABEL_0', 'score': 0.99997187, 'index': 9, 'word': 're', 'start': 34, 'end': 36}, {'entity': 'LABEL_0', 'score': 0.999957, 'index': 10, 'word': '##no', 'start': 36, 'end': 38}]\n"
     ]
    }
   ],
   "source": [
    "# nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# ner_results = nlp(sequence)\n",
    "# print(ner_results[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'The', 'man', 'for', 'wisdom', '’', 's', 'various', 'arts', 're']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(sequence, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1109,  1299,  1111, 12304,   787,   188,  1672,  3959,  1231])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.5953, -2.2641, -2.4301, -1.7081, -2.7219],\n",
       "        [ 1.7000,  0.4799, -3.5283, -2.5794,  3.4231],\n",
       "        [ 8.8453, -2.2387, -2.7023, -1.2831, -2.6234],\n",
       "        [ 9.2018, -2.4289, -2.6260, -1.5701, -3.0763],\n",
       "        [ 8.5895, -2.0201, -3.0749, -1.6693, -2.1723],\n",
       "        [ 9.1637, -2.3724, -2.5144, -1.6161, -3.0594],\n",
       "        [ 9.3578, -2.2696, -2.6097, -1.5840, -3.0662],\n",
       "        [ 9.2726, -2.4319, -2.6974, -1.5354, -2.9726],\n",
       "        [ 8.8622, -2.1812, -2.7793, -1.4634, -2.8141],\n",
       "        [ 9.4867, -2.3407, -2.5629, -1.8648, -3.1766]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.argmax(outputs, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS]', 'LABEL_0')\n",
      "('The', 'LABEL_4')\n",
      "('man', 'LABEL_0')\n",
      "('for', 'LABEL_0')\n",
      "('wisdom', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('s', 'LABEL_0')\n",
      "('various', 'LABEL_0')\n",
      "('arts', 'LABEL_0')\n",
      "('re', 'LABEL_0')\n",
      "('##no', 'LABEL_0')\n",
      "('##wn', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('Long', 'LABEL_4')\n",
      "('exercised', 'LABEL_0')\n",
      "('in', 'LABEL_0')\n",
      "('w', 'LABEL_0')\n",
      "('##oes', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('O', 'LABEL_0')\n",
      "('Muse', 'LABEL_4')\n",
      "('!', 'LABEL_0')\n",
      "('re', 'LABEL_0')\n",
      "('##sound', 'LABEL_0')\n",
      "(';', 'LABEL_0')\n",
      "('Who', 'LABEL_4')\n",
      "(',', 'LABEL_0')\n",
      "('when', 'LABEL_0')\n",
      "('his', 'LABEL_0')\n",
      "('arms', 'LABEL_0')\n",
      "('had', 'LABEL_0')\n",
      "('wrought', 'LABEL_0')\n",
      "('the', 'LABEL_0')\n",
      "('destined', 'LABEL_0')\n",
      "('fall', 'LABEL_0')\n",
      "('Of', 'LABEL_0')\n",
      "('sacred', 'LABEL_0')\n",
      "('Troy', 'LABEL_4')\n",
      "(',', 'LABEL_0')\n",
      "('and', 'LABEL_0')\n",
      "('r', 'LABEL_0')\n",
      "('##azed', 'LABEL_0')\n",
      "('her', 'LABEL_0')\n",
      "('heaven', 'LABEL_0')\n",
      "('-', 'LABEL_0')\n",
      "('built', 'LABEL_0')\n",
      "('wall', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('Wan', 'LABEL_1')\n",
      "('##dering', 'LABEL_3')\n",
      "('from', 'LABEL_0')\n",
      "('c', 'LABEL_0')\n",
      "('##lim', 'LABEL_0')\n",
      "('##e', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('c', 'LABEL_0')\n",
      "('##lim', 'LABEL_0')\n",
      "('##e', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('o', 'LABEL_0')\n",
      "('##bs', 'LABEL_0')\n",
      "('##er', 'LABEL_0')\n",
      "('##vant', 'LABEL_0')\n",
      "('stray', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('Their', 'LABEL_4')\n",
      "('manners', 'LABEL_0')\n",
      "('noted', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('and', 'LABEL_0')\n",
      "('their', 'LABEL_0')\n",
      "('states', 'LABEL_0')\n",
      "('survey', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('On', 'LABEL_4')\n",
      "('storm', 'LABEL_0')\n",
      "('##y', 'LABEL_0')\n",
      "('seas', 'LABEL_0')\n",
      "('un', 'LABEL_0')\n",
      "('##num', 'LABEL_0')\n",
      "('##ber', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('##ils', 'LABEL_0')\n",
      "('he', 'LABEL_0')\n",
      "('bore', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('Safe', 'LABEL_4')\n",
      "('with', 'LABEL_0')\n",
      "('his', 'LABEL_0')\n",
      "('friends', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('gain', 'LABEL_0')\n",
      "('his', 'LABEL_0')\n",
      "('na', 'LABEL_0')\n",
      "('##tal', 'LABEL_0')\n",
      "('shore', 'LABEL_0')\n",
      "(':', 'LABEL_0')\n",
      "('V', 'LABEL_1')\n",
      "('##ain', 'LABEL_3')\n",
      "('to', 'LABEL_0')\n",
      "('##ils', 'LABEL_0')\n",
      "('!', 'LABEL_0')\n",
      "('their', 'LABEL_0')\n",
      "('imp', 'LABEL_0')\n",
      "('##ious', 'LABEL_0')\n",
      "('f', 'LABEL_0')\n",
      "('##olly', 'LABEL_0')\n",
      "('dared', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('prey', 'LABEL_0')\n",
      "('On', 'LABEL_4')\n",
      "('herd', 'LABEL_0')\n",
      "('##s', 'LABEL_0')\n",
      "('devoted', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('the', 'LABEL_0')\n",
      "('god', 'LABEL_0')\n",
      "('of', 'LABEL_0')\n",
      "('day', 'LABEL_0')\n",
      "(';', 'LABEL_0')\n",
      "('The', 'LABEL_4')\n",
      "('god', 'LABEL_0')\n",
      "('v', 'LABEL_0')\n",
      "('##ind', 'LABEL_0')\n",
      "('##ict', 'LABEL_0')\n",
      "('##ive', 'LABEL_0')\n",
      "('doom', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "('them', 'LABEL_0')\n",
      "('never', 'LABEL_0')\n",
      "('more', 'LABEL_0')\n",
      "('(', 'LABEL_0')\n",
      "('Ah', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('men', 'LABEL_0')\n",
      "('un', 'LABEL_0')\n",
      "('##bles', 'LABEL_0')\n",
      "('##s', 'LABEL_0')\n",
      "('’', 'LABEL_0')\n",
      "('d', 'LABEL_0')\n",
      "('!', 'LABEL_0')\n",
      "(')', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('touch', 'LABEL_0')\n",
      "('that', 'LABEL_0')\n",
      "('na', 'LABEL_0')\n",
      "('##tal', 'LABEL_0')\n",
      "('shore', 'LABEL_0')\n",
      "('.', 'LABEL_0')\n",
      "('Oh', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('s', 'LABEL_0')\n",
      "('##natch', 'LABEL_0')\n",
      "('some', 'LABEL_0')\n",
      "('portion', 'LABEL_0')\n",
      "('of', 'LABEL_0')\n",
      "('these', 'LABEL_0')\n",
      "('acts', 'LABEL_0')\n",
      "('from', 'LABEL_0')\n",
      "('fate', 'LABEL_0')\n",
      "(',', 'LABEL_0')\n",
      "('that', 'LABEL_0')\n",
      "(\"'\", 'LABEL_0')\n",
      "('s', 'LABEL_0')\n",
      "('Ce', 'LABEL_1')\n",
      "('##les', 'LABEL_2')\n",
      "('##tial', 'LABEL_2')\n",
      "('Muse', 'LABEL_4')\n",
      "('!', 'LABEL_0')\n",
      "('and', 'LABEL_0')\n",
      "('to', 'LABEL_0')\n",
      "('our', 'LABEL_0')\n",
      "('world', 'LABEL_0')\n",
      "('relate', 'LABEL_0')\n",
      "('.', 'LABEL_0')\n",
      "('[SEP]', 'LABEL_0')\n"
     ]
    }
   ],
   "source": [
    "for token, prediction in zip(tokens, predictions[0].numpy()):\n",
    "    print((token, model.config.id2label[prediction]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'U-CLEntity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'U-CLEntity', 'O', 'O', 'O', 'O', 'O', 'O', 'U-CLEntity', 'O', 'O', 'O', 'O', 'U-CLEntity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'U-CLEntity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CLEntity', 'L-CLEntity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'U-CLEntity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'U-CLEntity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'U-CLEntity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CLEntity', 'L-CLEntity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'U-CLEntity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'U-CLEntity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CLEntity', 'I-CLEntity', 'I-CLEntity', 'U-CLEntity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "pred_line_label = []\n",
    "for prediction in predictions[0].numpy():\n",
    "    pred_line_label.append(label_list[prediction])\n",
    "pred.append(pred_line_label)\n",
    "print(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern = r' ([A-Z].[a-z]+)'\n",
    "pattern = r'(\\b[A-Z][a-z]+\\b)(\\s\\b[A-Z][a-z]+\\b)*'\n",
    "re.compile(pattern)\n",
    "\n",
    "def get_annotations(text, pattern):\n",
    "    annotations = []\n",
    "    for match in re.finditer(pattern, text):\n",
    "        label_dic = dict()\n",
    "        label_dic['start'] = match.start()\n",
    "        label_dic['end'] = match.end()\n",
    "        label_dic['label'] = 'CLEntity' # Entity starting with a capital letter\n",
    "        annotations.append(label_dic)\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_annotations = get_annotations(sequence, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try an exmaple\n",
    "from transformers import BertTokenizerFast, BatchEncoding\n",
    "from tokenizers import Encoding\n",
    "from alignment import align_tokens_and_annotations_bilou\n",
    "\n",
    "example = {'content': sequence, 'annotations': ex_annotations}\n",
    "tokenizer_ex = BertTokenizerFast.from_pretrained('bert-base-cased') # Load a pre-trained tokenizer\n",
    "tokenized_batch_ex : BatchEncoding = tokenizer_ex(example[\"content\"])\n",
    "tokenized_text : Encoding = tokenized_batch_ex[0]\n",
    "labels = align_tokens_and_annotations_bilou(tokenized_text, example[\"annotations\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] - 0\n",
      "The - 4\n",
      "man - 0\n",
      "for - 0\n",
      "wisdom - 0\n",
      "’ - 0\n",
      "s - 0\n",
      "various - 0\n",
      "arts - 0\n",
      "re - 0\n",
      "##no - 0\n",
      "##wn - 0\n",
      "’ - 0\n",
      "d - 0\n",
      ", - 0\n",
      "Long - 4\n",
      "exercised - 0\n",
      "in - 0\n",
      "w - 0\n",
      "##oes - 0\n",
      ", - 0\n",
      "O - 0\n",
      "Muse - 4\n",
      "! - 0\n",
      "re - 0\n",
      "##sound - 0\n",
      "; - 0\n",
      "Who - 4\n",
      ", - 0\n",
      "when - 0\n",
      "his - 0\n",
      "arms - 0\n",
      "had - 0\n",
      "wrought - 0\n",
      "the - 0\n",
      "destined - 0\n",
      "fall - 0\n",
      "Of - 4\n",
      "sacred - 0\n",
      "Troy - 4\n",
      ", - 0\n",
      "and - 0\n",
      "r - 0\n",
      "##azed - 0\n",
      "her - 0\n",
      "heaven - 0\n",
      "- - 0\n",
      "built - 0\n",
      "wall - 0\n",
      ", - 0\n",
      "Wan - 1\n",
      "##dering - 3\n",
      "from - 0\n",
      "c - 0\n",
      "##lim - 0\n",
      "##e - 0\n",
      "to - 0\n",
      "c - 0\n",
      "##lim - 0\n",
      "##e - 0\n",
      ", - 0\n",
      "o - 0\n",
      "##bs - 0\n",
      "##er - 0\n",
      "##vant - 0\n",
      "stray - 0\n",
      "’ - 0\n",
      "d - 0\n",
      ", - 0\n",
      "Their - 4\n",
      "manners - 0\n",
      "noted - 0\n",
      ", - 0\n",
      "and - 0\n",
      "their - 0\n",
      "states - 0\n",
      "survey - 0\n",
      "’ - 0\n",
      "d - 0\n",
      ", - 0\n",
      "On - 4\n",
      "storm - 0\n",
      "##y - 0\n",
      "seas - 0\n",
      "un - 0\n",
      "##num - 0\n",
      "##ber - 0\n",
      "’ - 0\n",
      "d - 0\n",
      "to - 0\n",
      "##ils - 0\n",
      "he - 0\n",
      "bore - 0\n",
      ", - 0\n",
      "Safe - 4\n",
      "with - 0\n",
      "his - 0\n",
      "friends - 0\n",
      "to - 0\n",
      "gain - 0\n",
      "his - 0\n",
      "na - 0\n",
      "##tal - 0\n",
      "shore - 0\n",
      ": - 0\n",
      "V - 1\n",
      "##ain - 3\n",
      "to - 0\n",
      "##ils - 0\n",
      "! - 0\n",
      "their - 0\n",
      "imp - 0\n",
      "##ious - 0\n",
      "f - 0\n",
      "##olly - 0\n",
      "dared - 0\n",
      "to - 0\n",
      "prey - 0\n",
      "On - 4\n",
      "herd - 0\n",
      "##s - 0\n",
      "devoted - 0\n",
      "to - 0\n",
      "the - 0\n",
      "god - 0\n",
      "of - 0\n",
      "day - 0\n",
      "; - 0\n",
      "The - 4\n",
      "god - 0\n",
      "v - 0\n",
      "##ind - 0\n",
      "##ict - 0\n",
      "##ive - 0\n",
      "doom - 0\n",
      "’ - 0\n",
      "d - 0\n",
      "them - 0\n",
      "never - 0\n",
      "more - 0\n",
      "( - 0\n",
      "Ah - 4\n",
      ", - 0\n",
      "men - 0\n",
      "un - 0\n",
      "##bles - 0\n",
      "##s - 0\n",
      "’ - 0\n",
      "d - 0\n",
      "! - 0\n",
      ") - 0\n",
      "to - 0\n",
      "touch - 0\n",
      "that - 0\n",
      "na - 0\n",
      "##tal - 0\n",
      "shore - 0\n",
      ". - 0\n",
      "Oh - 4\n",
      ", - 0\n",
      "s - 0\n",
      "##natch - 0\n",
      "some - 0\n",
      "portion - 0\n",
      "of - 0\n",
      "these - 0\n",
      "acts - 0\n",
      "from - 0\n",
      "fate - 0\n",
      ", - 0\n",
      "that - 0\n",
      "' - 0\n",
      "s - 0\n",
      "Ce - 1\n",
      "##les - 2\n",
      "##tial - 2\n",
      "Muse - 3\n",
      "! - 0\n",
      "and - 0\n",
      "to - 0\n",
      "our - 0\n",
      "world - 0\n",
      "relate - 0\n",
      ". - 0\n",
      "[SEP] - 0\n"
     ]
    }
   ],
   "source": [
    "from labelset import LabelSet\n",
    "\n",
    "ex_label_set = LabelSet(labels=[\"CLEntity\"])\n",
    "aligned_label_ids = ex_label_set.get_aligned_label_ids_from_annotations(\n",
    "    tokenized_text, example[\"annotations\"]\n",
    ")\n",
    "tokens = tokenized_text.tokens\n",
    "\n",
    "for token, label in zip(tokens, aligned_label_ids):\n",
    "    print(token, \"-\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'U-CLEntity', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "true = []\n",
    "true_line_label = []\n",
    "for label in aligned_label_ids:\n",
    "    true_line_label.append(label_list[label])\n",
    "true.append(true_line_label)\n",
    "print(true[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    CLEntity       0.92      0.75      0.83        16\n",
      "\n",
      "   micro avg       0.92      0.75      0.83        16\n",
      "   macro avg       0.92      0.75      0.83        16\n",
      "weighted avg       0.92      0.75      0.83        16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score, f1_score, classification_report, precision_score, recall_score\n",
    "from seqeval.scheme import BILOU\n",
    "\n",
    "print(classification_report(true, pred, mode='strict', scheme=BILOU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall = recall_score(true, pred)\n",
    "# precision_score(true, pred)\n",
    "# from matplotlib import pyplot\n",
    "# # plot the model precision-recall curve\n",
    "# pyplot.plot(recall, precision, marker='.')\n",
    "# # axis labels\n",
    "# pyplot.xlabel('Recall')\n",
    "# pyplot.ylabel('Precision')\n",
    "# # show the legend\n",
    "# pyplot.legend()\n",
    "# # show the plot\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://huggingface.co/docs/tokenizers/python/latest/pipeline.html\n",
    "# tokenizer.decoder = decoders.WordPiece()\n",
    "# tokenizer.decode(outputs.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odyssey_lines = []\n",
    "book = open(\"../example-texts/odyssey.txt\")\n",
    "for line in book:\n",
    "    line = line.strip()\n",
    "    odyssey_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odyssey_lines = [line for line in odyssey_lines if line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odyssey_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## https://huggingface.co/docs/tokenizers/python/latest/pipeline.html\n",
    "# tokenizer.decoder = decoders.WordPiece()\n",
    "# tokenizer.decode(outputs.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in odyssey_lines:\n",
    "    tokens_line = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(line)))\n",
    "    inputs_line = tokenizer.encode(line, return_tensors=\"pt\")\n",
    "\n",
    "    outputs_line = model(inputs_line).logits\n",
    "    predictions_line = torch.argmax(outputs_line, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = []\n",
    "book = open(\"../example-texts/odyssey.txt\")\n",
    "for line in book:\n",
    "    line = line.strip()\n",
    "    \n",
    "    line_data = dict()\n",
    "    line_data['content'] = line\n",
    "    line_data['annotations'] = get_annotations(line, pattern)\n",
    "    json_data.append(line_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing the result. Need a way to test whether tuned bert is accurately identifying CLEntities\n",
    "json_data[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda (base)",
   "language": "python",
   "name": "anaconda-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
