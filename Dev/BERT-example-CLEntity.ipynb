{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List,Any\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IntList = List[int] # A list of token_ids\n",
    "IntListList = List[IntList] # A List of List of token_ids, e.g. a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'(\\b[A-Z][a-z]+\\b)(\\s\\b[A-Z][a-z]+\\b)*'\n",
    "re.compile(pattern)\n",
    "\n",
    "def get_annotations(text, pattern):\n",
    "    annotations = []\n",
    "    for match in re.finditer(pattern, text):\n",
    "        label_dic = dict()\n",
    "        label_dic['start'] = match.start()\n",
    "        label_dic['end'] = match.end()\n",
    "        label_dic['label'] = 'CLEntity' # Entity starting with a capital letter\n",
    "        annotations.append(label_dic)\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'STOP', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Early Journal Content on JSTOR, Free to Anyone in the World', 'annotations': [{'start': 0, 'end': 21, 'label': 'CLEntity'}, {'start': 32, 'end': 36, 'label': 'CLEntity'}, {'start': 40, 'end': 46, 'label': 'CLEntity'}, {'start': 54, 'end': 59, 'label': 'CLEntity'}]}, {'content': '', 'annotations': []}, {'content': 'This article is one of nearly 500,000 scholarly works digitized and made freely available to everyone in', 'annotations': [{'start': 0, 'end': 4, 'label': 'CLEntity'}]}, {'content': 'the world by JSTOR.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Known as the Early Journal Content, this set of works include research articles, news, letters, and other', 'annotations': [{'start': 0, 'end': 5, 'label': 'CLEntity'}, {'start': 13, 'end': 34, 'label': 'CLEntity'}]}, {'content': 'writings published in more than 200 of the oldest leading academic journals. The works date from the', 'annotations': [{'start': 77, 'end': 80, 'label': 'CLEntity'}]}, {'content': 'mid-seventeenth to the early twentieth centuries.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'We encourage people to read and share the Early Journal Content openly and to tell others that this', 'annotations': [{'start': 0, 'end': 2, 'label': 'CLEntity'}, {'start': 42, 'end': 63, 'label': 'CLEntity'}]}, {'content': 'resource exists. People may post this content online or redistribute in any way for non-commercial', 'annotations': [{'start': 17, 'end': 23, 'label': 'CLEntity'}]}, {'content': 'purposes.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Read more about Early Journal Content at http://about.jstor.org/participate-jstor/individuals/early-', 'annotations': [{'start': 0, 'end': 4, 'label': 'CLEntity'}, {'start': 16, 'end': 37, 'label': 'CLEntity'}]}, {'content': 'journal-content .', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'JSTOR is a digital library of academic journals, books, and primary source objects. JSTOR helps people', 'annotations': []}, {'content': 'discover, use, and build upon a wide range of content through a powerful research and teaching', 'annotations': []}, {'content': 'platform, and preserves this content for future generations. JSTOR is part of ITHAKA, a not-for-profit', 'annotations': []}, {'content': 'organization that also includes Ithaka S+R and Portico. For more information about JSTOR, please', 'annotations': [{'start': 32, 'end': 38, 'label': 'CLEntity'}, {'start': 47, 'end': 54, 'label': 'CLEntity'}, {'start': 56, 'end': 59, 'label': 'CLEntity'}]}, {'content': 'contact support@jstor.org.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'April 25, 1921)', 'annotations': [{'start': 0, 'end': 5, 'label': 'CLEntity'}]}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'THE CLASSICAL. WEEKLY', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '183', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'reveals a long, loving, and intimate association with', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'the best Latin masters, which, when combined with', 'annotations': [{'start': 9, 'end': 14, 'label': 'CLEntity'}]}, {'content': '', 'annotations': []}, {'content': 'sound critical sense, commends their literary judgments.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Union College,', 'annotations': [{'start': 0, 'end': 13, 'label': 'CLEntity'}]}, {'content': 'Schenectady, N. Y.', 'annotations': [{'start': 0, 'end': 11, 'label': 'CLEntity'}]}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'George Dwight Kellogg.', 'annotations': [{'start': 0, 'end': 21, 'label': 'CLEntity'}]}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Thucydides, Book IV, Chapters I-XLI (Pylus and', 'annotations': [{'start': 0, 'end': 10, 'label': 'CLEntity'}, {'start': 12, 'end': 16, 'label': 'CLEntity'}, {'start': 21, 'end': 29, 'label': 'CLEntity'}, {'start': 37, 'end': 42, 'label': 'CLEntity'}]}, {'content': 'Sphacteria). Edited by J. H. E. Crees and J. C.', 'annotations': [{'start': 0, 'end': 10, 'label': 'CLEntity'}, {'start': 13, 'end': 19, 'label': 'CLEntity'}, {'start': 32, 'end': 37, 'label': 'CLEntity'}]}, {'content': 'Wordsworth. Cambridge: at the University Press', 'annotations': [{'start': 0, 'end': 10, 'label': 'CLEntity'}, {'start': 12, 'end': 21, 'label': 'CLEntity'}, {'start': 30, 'end': 46, 'label': 'CLEntity'}]}, {'content': '(1919). Pp. xvi + 96.', 'annotations': [{'start': 8, 'end': 10, 'label': 'CLEntity'}]}, {'content': 'In a brief Preface it is stated that this edition has', 'annotations': [{'start': 0, 'end': 2, 'label': 'CLEntity'}, {'start': 11, 'end': 18, 'label': 'CLEntity'}]}, {'content': 'been prepared for those who have not long been study-', 'annotations': []}, {'content': 'ing Greek and who have reached the stage of the', 'annotations': [{'start': 4, 'end': 9, 'label': 'CLEntity'}]}, {'content': '\"First School Examination \" . A vocabulary has therefore', 'annotations': [{'start': 1, 'end': 25, 'label': 'CLEntity'}]}, {'content': 'been added (72-96). The book contains also a map of', 'annotations': [{'start': 20, 'end': 23, 'label': 'CLEntity'}]}, {'content': 'Pylus and Sphacteria. The Introduction (ix-xv) deals', 'annotations': [{'start': 0, 'end': 5, 'label': 'CLEntity'}, {'start': 10, 'end': 20, 'label': 'CLEntity'}, {'start': 22, 'end': 38, 'label': 'CLEntity'}]}, {'content': 'with the life and work of Thucydides and the contents', 'annotations': [{'start': 26, 'end': 36, 'label': 'CLEntity'}]}, {'content': 'of his history, Book IV, Chapters I-XLI. Mr. Crees,', 'annotations': [{'start': 16, 'end': 20, 'label': 'CLEntity'}, {'start': 25, 'end': 33, 'label': 'CLEntity'}, {'start': 41, 'end': 43, 'label': 'CLEntity'}, {'start': 45, 'end': 50, 'label': 'CLEntity'}]}, {'content': 'author of the Introduction, writes enthusiastically of', 'annotations': [{'start': 14, 'end': 26, 'label': 'CLEntity'}]}, {'content': \"Thucydides as a historian. Thucydides's history, he\", 'annotations': [{'start': 0, 'end': 10, 'label': 'CLEntity'}, {'start': 27, 'end': 37, 'label': 'CLEntity'}]}, {'content': 'says, \"would at any time have been a great work, but', 'annotations': []}, {'content': 'for its date it is in its conception a marvellous achieve-', 'annotations': []}, {'content': 'ment, and the expression of a person lity which', 'annotations': []}, {'content': 'compels respect\". Thucydides necessarily, as a true', 'annotations': [{'start': 18, 'end': 28, 'label': 'CLEntity'}]}, {'content': 'Athenian, was a partisan, but he was none the less able', 'annotations': [{'start': 0, 'end': 8, 'label': 'CLEntity'}]}, {'content': 'to efface his partisanship and \"achieved a monumental', 'annotations': []}, {'content': 'impartiality\". Thucydides, the aristocrat, is so fair to', 'annotations': [{'start': 15, 'end': 25, 'label': 'CLEntity'}]}, {'content': 'Cleon that \"the champions of Cleon must, and can', 'annotations': [{'start': 0, 'end': 5, 'label': 'CLEntity'}, {'start': 29, 'end': 34, 'label': 'CLEntity'}]}, {'content': 'base their championship on the evidence of Thucydi-', 'annotations': [{'start': 43, 'end': 50, 'label': 'CLEntity'}]}, {'content': 'des\". There are 38 pages of notes to 33 of the Greek', 'annotations': [{'start': 6, 'end': 11, 'label': 'CLEntity'}, {'start': 47, 'end': 52, 'label': 'CLEntity'}]}, {'content': 'text. Yet, I suspect, in more than one place, the stu-', 'annotations': [{'start': 6, 'end': 9, 'label': 'CLEntity'}]}, {'content': 'dent who has not \"long been studying Greek\" would', 'annotations': [{'start': 37, 'end': 42, 'label': 'CLEntity'}]}, {'content': 'need more assistance than the authors give him toward', 'annotations': []}, {'content': 'the interpretation of the text. c. K.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'A NOTE ON THE RED RAIN IN ILIAD 16.459', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'While listening recently to some lectures of Professor', 'annotations': [{'start': 0, 'end': 5, 'label': 'CLEntity'}, {'start': 45, 'end': 54, 'label': 'CLEntity'}]}, {'content': 'David M. Robinson on Homer, I was led to ask: Are', 'annotations': [{'start': 0, 'end': 5, 'label': 'CLEntity'}, {'start': 9, 'end': 17, 'label': 'CLEntity'}, {'start': 21, 'end': 26, 'label': 'CLEntity'}, {'start': 46, 'end': 49, 'label': 'CLEntity'}]}, {'content': 'the following passages of the Iliad merely the product', 'annotations': [{'start': 30, 'end': 35, 'label': 'CLEntity'}]}, {'content': \"of the poet's imagination or do they refer to real\", 'annotations': []}, {'content': 'natural phenomena?', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'In Iliad 16.459 we read 1', 'annotations': [{'start': 0, 'end': 8, 'label': 'CLEntity'}]}, {'content': '', 'annotations': []}, {'content': 'She spoke nor did the sire of Gods and men', 'annotations': [{'start': 0, 'end': 3, 'label': 'CLEntity'}, {'start': 30, 'end': 34, 'label': 'CLEntity'}]}, {'content': 'Unheeding hear, but poured down on the earth', 'annotations': [{'start': 0, 'end': 9, 'label': 'CLEntity'}]}, {'content': 'Rain drops of blood, so honoring his dear son,', 'annotations': [{'start': 0, 'end': 4, 'label': 'CLEntity'}]}, {'content': 'Him whom Patroclus was foredoomed to slay', 'annotations': [{'start': 0, 'end': 3, 'label': 'CLEntity'}, {'start': 9, 'end': 18, 'label': 'CLEntity'}]}, {'content': \"In Troy's rich soil far from his native land.\", 'annotations': [{'start': 0, 'end': 7, 'label': 'CLEntity'}]}, {'content': '', 'annotations': []}, {'content': 'In Iliad 11.54 we read:', 'annotations': [{'start': 0, 'end': 8, 'label': 'CLEntity'}]}, {'content': '', 'annotations': []}, {'content': 'Zeus roused an evil blare of war and sent', 'annotations': [{'start': 0, 'end': 4, 'label': 'CLEntity'}]}, {'content': 'Down from high heaven his rain drops stained with', 'annotations': [{'start': 0, 'end': 4, 'label': 'CLEntity'}]}, {'content': 'blood.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Again in Hesiod, Shield of Heracles 383-385, in a', 'annotations': [{'start': 0, 'end': 5, 'label': 'CLEntity'}, {'start': 9, 'end': 15, 'label': 'CLEntity'}, {'start': 17, 'end': 23, 'label': 'CLEntity'}, {'start': 27, 'end': 35, 'label': 'CLEntity'}]}, {'content': 'passage perhaps imitated from the above, we have:', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Loud thundered Zeus, the counselor, flinging down', 'annotations': [{'start': 0, 'end': 4, 'label': 'CLEntity'}, {'start': 15, 'end': 19, 'label': 'CLEntity'}]}, {'content': 'From heaven bloody rain drops, setting thus', 'annotations': [{'start': 0, 'end': 4, 'label': 'CLEntity'}]}, {'content': 'A sign of battle to his great-souled son.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'In short, is there such a thing as red rain, apart from', 'annotations': [{'start': 0, 'end': 2, 'label': 'CLEntity'}]}, {'content': 'the effluvia of butterflies as suggested by Buchholz,', 'annotations': [{'start': 44, 'end': 52, 'label': 'CLEntity'}]}, {'content': 'Die Homerische Realien, 3.91, and is Homer justified', 'annotations': [{'start': 0, 'end': 22, 'label': 'CLEntity'}, {'start': 37, 'end': 42, 'label': 'CLEntity'}]}, {'content': 'in the use he makes of it? That there is and that Homer', 'annotations': [{'start': 27, 'end': 31, 'label': 'CLEntity'}, {'start': 50, 'end': 55, 'label': 'CLEntity'}]}, {'content': 'is better acquainted with and truer to nature than', 'annotations': []}, {'content': 'some of his critics is shown by the following note', 'annotations': []}, {'content': \"appended to certain verses of John Ruskin's Poem,\", 'annotations': [{'start': 30, 'end': 41, 'label': 'CLEntity'}, {'start': 44, 'end': 48, 'label': 'CLEntity'}]}, {'content': 'The Broken Chain (Geo. Allen, Library Edition of', 'annotations': [{'start': 0, 'end': 16, 'label': 'CLEntity'}, {'start': 18, 'end': 21, 'label': 'CLEntity'}, {'start': 23, 'end': 28, 'label': 'CLEntity'}, {'start': 30, 'end': 45, 'label': 'CLEntity'}]}, {'content': 'John Ruskin, 2.177 [1903I). The verses are:', 'annotations': [{'start': 0, 'end': 11, 'label': 'CLEntity'}, {'start': 28, 'end': 31, 'label': 'CLEntity'}]}, {'content': '', 'annotations': []}, {'content': 'Like purple-rain at evening shed', 'annotations': [{'start': 0, 'end': 4, 'label': 'CLEntity'}]}, {'content': \"On Sestri's cedar-darkened shore.\", 'annotations': [{'start': 0, 'end': 9, 'label': 'CLEntity'}]}, {'content': '', 'annotations': []}, {'content': 'The note runs thus:', 'annotations': [{'start': 0, 'end': 3, 'label': 'CLEntity'}]}, {'content': 'I never saw such a thing but once, on the mountains of', 'annotations': []}, {'content': 'Sestri in the Gulf of Genoa. The whole western half', 'annotations': [{'start': 0, 'end': 6, 'label': 'CLEntity'}, {'start': 14, 'end': 18, 'label': 'CLEntity'}, {'start': 22, 'end': 27, 'label': 'CLEntity'}, {'start': 29, 'end': 32, 'label': 'CLEntity'}]}, {'content': 'of the sky was one intense ambei colour, the air crystal-', 'annotations': []}, {'content': 'line and cloudless, the other half grey with drifting', 'annotations': []}, {'content': 'showers. At the instant of sunset, the whole mass of', 'annotations': [{'start': 9, 'end': 11, 'label': 'CLEntity'}]}, {'content': 'rain turned of a deep rose-colour, the consequent rain-', 'annotations': []}, {'content': 'bow being not varied with the seven colours, but one', 'annotations': []}, {'content': 'broad belt of paler rose; the other tints being so deli-', 'annotations': []}, {'content': 'cate as to be overwhelmed by the crimson of the rain.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'I have myself witnessed red rain in Chatham, Massa-', 'annotations': [{'start': 36, 'end': 43, 'label': 'CLEntity'}, {'start': 45, 'end': 50, 'label': 'CLEntity'}]}, {'content': 'chusetts, over Nantucket Sound. When one lives in', 'annotations': [{'start': 15, 'end': 30, 'label': 'CLEntity'}, {'start': 32, 'end': 36, 'label': 'CLEntity'}]}, {'content': 'the country and on the sea year in and year out, one', 'annotations': []}, {'content': 'acquires a wholesome respect for the observing powers', 'annotations': []}, {'content': 'of the classical poets, notably Homer. The phe-', 'annotations': [{'start': 32, 'end': 37, 'label': 'CLEntity'}, {'start': 39, 'end': 42, 'label': 'CLEntity'}]}, {'content': 'nomenon occurred at sunset, with drifting curtains of', 'annotations': []}, {'content': 'rain between the observer and the sun. These the red', 'annotations': [{'start': 39, 'end': 44, 'label': 'CLEntity'}]}, {'content': 'rays of the sinking orb shot through and through with', 'annotations': []}, {'content': 'deep crimson that faded and revived as the curtains of', 'annotations': []}, {'content': 'rain fell and succeeded one another. There was no', 'annotations': [{'start': 37, 'end': 42, 'label': 'CLEntity'}]}, {'content': 'rainbow, as the rain was between the observer and the', 'annotations': []}, {'content': 'sun, for one always sees a rainbow when he is between', 'annotations': []}, {'content': 'the rain and the sun (or the moon, in the case of a lunar', 'annotations': []}, {'content': 'rainbow, which is very rare), and of course the luminary', 'annotations': []}, {'content': 'cannot be very high in the heavens in either case.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': \"Given such a phenomenon, Homer's application is\", 'annotations': [{'start': 0, 'end': 5, 'label': 'CLEntity'}, {'start': 25, 'end': 30, 'label': 'CLEntity'}]}, {'content': 'obvious and justified, as the following quotations from', 'annotations': []}, {'content': 'Byron, Sardanapalus, and Turner, Fallacies of Hope,', 'annotations': [{'start': 0, 'end': 5, 'label': 'CLEntity'}, {'start': 7, 'end': 19, 'label': 'CLEntity'}, {'start': 25, 'end': 31, 'label': 'CLEntity'}, {'start': 33, 'end': 42, 'label': 'CLEntity'}, {'start': 46, 'end': 50, 'label': 'CLEntity'}]}, {'content': 'prove. In Byron, the Chaldean priest says of the', 'annotations': [{'start': 7, 'end': 15, 'label': 'CLEntity'}, {'start': 21, 'end': 29, 'label': 'CLEntity'}]}, {'content': 'sinking sun:', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'How red he glares amidst those deepening clouds,', 'annotations': [{'start': 0, 'end': 3, 'label': 'CLEntity'}]}, {'content': 'Like the blood he predicts.', 'annotations': [{'start': 0, 'end': 4, 'label': 'CLEntity'}]}, {'content': '', 'annotations': []}, {'content': \"Turner's lines were over a picture of The Fall of\", 'annotations': [{'start': 0, 'end': 6, 'label': 'CLEntity'}, {'start': 38, 'end': 46, 'label': 'CLEntity'}]}, {'content': 'Carthage:', 'annotations': [{'start': 0, 'end': 8, 'label': 'CLEntity'}]}, {'content': '', 'annotations': []}, {'content': \"While o'er the western wave the ensanguined sun\", 'annotations': [{'start': 0, 'end': 5, 'label': 'CLEntity'}]}, {'content': 'Is gathering huge a stormy signal spread,', 'annotations': [{'start': 0, 'end': 2, 'label': 'CLEntity'}]}, {'content': 'And set portentous.', 'annotations': [{'start': 0, 'end': 3, 'label': 'CLEntity'}]}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Rogers Fellow in Classics,', 'annotations': [{'start': 0, 'end': 13, 'label': 'CLEntity'}, {'start': 17, 'end': 25, 'label': 'CLEntity'}]}, {'content': 'The Johns Hopkins University.', 'annotations': [{'start': 0, 'end': 28, 'label': 'CLEntity'}]}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Carol Wight.', 'annotations': [{'start': 0, 'end': 11, 'label': 'CLEntity'}]}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': \"'The translations are my own. So are the italics in the passages\", 'annotations': [{'start': 1, 'end': 4, 'label': 'CLEntity'}, {'start': 30, 'end': 32, 'label': 'CLEntity'}]}, {'content': 'quoted.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'CICERO, CATILINAM 2.4, ITERUM', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Utinam ille omnis secum suas copias eduxisset!', 'annotations': [{'start': 0, 'end': 6, 'label': 'CLEntity'}]}, {'content': 'Tongilium mihi eduxit, quern amare in praetexta', 'annotations': [{'start': 0, 'end': 9, 'label': 'CLEntity'}]}, {'content': 'coeperat; Publicium et Minucium, quorum aes', 'annotations': [{'start': 10, 'end': 19, 'label': 'CLEntity'}, {'start': 23, 'end': 31, 'label': 'CLEntity'}]}, {'content': 'alienum contractum in popina nullum rei publicae', 'annotations': []}, {'content': 'motum afferre poterat, reliquit. Quos viros! quanto', 'annotations': [{'start': 33, 'end': 37, 'label': 'CLEntity'}]}, {'content': 'aerealieno! quam valentis! quamnobilis!', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Until Professor Herrouet endeavored, in The', 'annotations': [{'start': 0, 'end': 24, 'label': 'CLEntity'}, {'start': 40, 'end': 43, 'label': 'CLEntity'}]}, {'content': 'Classical Weekly 14. 87, to refute my punctuation', 'annotations': [{'start': 0, 'end': 16, 'label': 'CLEntity'}]}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}]\n"
     ]
    }
   ],
   "source": [
    "json_data = []\n",
    "book = open(\"./example-texts/red-rain-in-iliad.txt\")\n",
    "for line in book:\n",
    "    line = line.strip()\n",
    "    \n",
    "    line_data = dict()\n",
    "    line_data['content'] = line\n",
    "    line_data['annotations'] = get_annotations(line, pattern)\n",
    "    json_data.append(line_data)\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('example-texts/iliad.txt') as fo:\n",
    "#     text = fo.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations = []\n",
    "# for match in re.finditer(pattern, text):\n",
    "#     label_dic = dict()\n",
    "#     label_dic['start'] = match.start()\n",
    "#     label_dic['end'] = match.end()\n",
    "#     label_dic['text'] = text[match.start():match.end()]\n",
    "#     label_dic['label'] = 'CL-Entity' # Entity starting with a capital letter\n",
    "#     annotations.append(label_dic)\n",
    "# print(len(annotations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast,  BatchEncoding\n",
    "from tokenizers import Encoding\n",
    "\n",
    "def align_tokens_and_annotations_bilou(tokenized: Encoding, annotations):\n",
    "    tokens = tokenized.tokens\n",
    "    aligned_labels = [\"O\"] * len(\n",
    "        tokens\n",
    "    )  # Make a list to store our labels the same length as our tokens\n",
    "    for anno in annotations:\n",
    "        annotation_token_ix_set = set()# A set that stores the token indices of the annotation\n",
    "        for char_ix in range(anno[\"start\"], anno[\"end\"]):\n",
    "\n",
    "            token_ix = tokenized.char_to_token(char_ix)\n",
    "            if token_ix is not None:\n",
    "                annotation_token_ix_set.add(token_ix)\n",
    "\n",
    "        if len(annotation_token_ix_set) == 1:\n",
    "            # If there is only one token\n",
    "            token_ix = annotation_token_ix_set.pop()\n",
    "            prefix = (\n",
    "                \"U\"  # This annotation spans one token so is prefixed with U for unique\n",
    "            )\n",
    "            aligned_labels[token_ix] = f\"{prefix}-{anno['label']}\"\n",
    "\n",
    "        else:\n",
    "\n",
    "            last_token_in_anno_ix = len(annotation_token_ix_set) - 1\n",
    "            for num, token_ix in enumerate(sorted(annotation_token_ix_set)):\n",
    "                if num == 0:\n",
    "                    prefix = \"B\"\n",
    "                elif num == last_token_in_anno_ix:\n",
    "                    prefix = \"L\"  # Its the last token\n",
    "                else:\n",
    "                    prefix = \"I\"  # We're inside of a multi token annotation\n",
    "                aligned_labels[token_ix] = f\"{prefix}-{anno['label']}\"\n",
    "    return aligned_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] - O\n",
      "We - U-CLEntity\n",
      "encourage - O\n",
      "people - O\n",
      "to - O\n",
      "read - O\n",
      "and - O\n",
      "share - O\n",
      "the - O\n",
      "Early - B-CLEntity\n",
      "Journal - I-CLEntity\n",
      "Content - L-CLEntity\n",
      "openly - O\n",
      "and - O\n",
      "to - O\n",
      "tell - O\n",
      "others - O\n",
      "that - O\n",
      "this - O\n",
      "[SEP] - O\n"
     ]
    }
   ],
   "source": [
    "# try an exmaple\n",
    "example = {'content': 'We encourage people to read and share the Early Journal Content openly and to tell others that this', 'annotations': [{'start': 0, 'end': 2, 'label': 'CLEntity'}, {'start': 42, 'end': 63, 'label': 'CLEntity'}]}\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased') # Load a pre-trained tokenizer\n",
    "tokenized_batch : BatchEncoding = tokenizer(example[\"content\"])\n",
    "tokenized_text : Encoding = tokenized_batch[0]\n",
    "labels = align_tokens_and_annotations_bilou(tokenized_text, example[\"annotations\"])\n",
    "\n",
    "for token, label in zip(tokenized_text.tokens, labels):\n",
    "    print(token, \"-\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] - 0\n",
      "We - 4\n",
      "encourage - 0\n",
      "people - 0\n",
      "to - 0\n",
      "read - 0\n",
      "and - 0\n",
      "share - 0\n",
      "the - 0\n",
      "Early - 1\n",
      "Journal - 2\n",
      "Content - 3\n",
      "openly - 0\n",
      "and - 0\n",
      "to - 0\n",
      "tell - 0\n",
      "others - 0\n",
      "that - 0\n",
      "this - 0\n",
      "[SEP] - 0\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "class LabelSet:\n",
    "    def __init__(self, labels: List[str]):\n",
    "        self.labels_to_id = {}\n",
    "        self.ids_to_label = {}\n",
    "        self.labels_to_id[\"O\"] = 0\n",
    "        self.ids_to_label[0] = \"O\"\n",
    "        num = 0  # in case there are no labels\n",
    "        # Writing BILU will give us incremntal ids for the labels\n",
    "        for _num, (label, s) in enumerate(itertools.product(labels, \"BILU\")):\n",
    "            num = _num + 1  # skip 0\n",
    "            l = f\"{s}-{label}\"\n",
    "            self.labels_to_id[l] = num\n",
    "            self.ids_to_label[num] = l\n",
    "        # Add the OUTSIDE label - no label for the token\n",
    "\n",
    "    def get_aligned_label_ids_from_annotations(self, tokenized_text, annotations):\n",
    "        raw_labels = align_tokens_and_annotations_bilou(tokenized_text, annotations)\n",
    "        return list(map(self.labels_to_id.get, raw_labels))\n",
    "\n",
    "\n",
    "example_label_set = LabelSet(labels=[\"CLEntity\"])\n",
    "aligned_label_ids = example_label_set.get_aligned_label_ids_from_annotations(\n",
    "    tokenized_text, example[\"annotations\"]\n",
    ")\n",
    "tokens = tokenized_text.tokens\n",
    "for token, label in zip(tokens, aligned_label_ids):\n",
    "    print(token, \"-\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingExample:\n",
    "    input_ids: IntList\n",
    "    attention_masks: IntList\n",
    "    labels: IntList\n",
    "\n",
    "\n",
    "class TraingDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: Any,\n",
    "        label_set: LabelSet,\n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        tokens_per_batch=32,\n",
    "        window_stride=None,\n",
    "    ):\n",
    "        self.label_set = label_set\n",
    "        if window_stride is None:\n",
    "            self.window_stride = tokens_per_batch\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = []\n",
    "        self.annotations = []\n",
    "\n",
    "        for example in data:\n",
    "            self.texts.append(example[\"content\"])\n",
    "            self.annotations.append(example[\"annotations\"])\n",
    "        ###TOKENIZE All THE DATA\n",
    "        tokenized_batch = self.tokenizer(self.texts, add_special_tokens=False)\n",
    "        ###ALIGN LABELS ONE EXAMPLE AT A TIME\n",
    "        aligned_labels = []\n",
    "        for ix in range(len(tokenized_batch.encodings)):\n",
    "            encoding = tokenized_batch.encodings[ix]\n",
    "            raw_annotations = self.annotations[ix]\n",
    "            aligned = label_set.get_aligned_label_ids_from_annotations(\n",
    "                encoding, raw_annotations\n",
    "            )\n",
    "            aligned_labels.append(aligned)\n",
    "        ###END OF LABEL ALIGNMENT\n",
    "\n",
    "        ###MAKE A LIST OF TRAINING EXAMPLES. (This is where we add padding)\n",
    "        self.training_examples: List[TrainingExample] = []\n",
    "        empty_label_id = \"O\"\n",
    "        for encoding, label in zip(tokenized_batch.encodings, aligned_labels):\n",
    "            length = len(label)  # How long is this sequence\n",
    "            for start in range(0, length, self.window_stride):\n",
    "\n",
    "                end = min(start + tokens_per_batch, length)\n",
    "\n",
    "                # How much padding do we need ?\n",
    "                padding_to_add = max(0, tokens_per_batch - end + start)\n",
    "                self.training_examples.append(\n",
    "                    TrainingExample(\n",
    "                        # Record the tokens\n",
    "                        input_ids=encoding.ids[start:end]  # The ids of the tokens\n",
    "                        + [self.tokenizer.pad_token_id]\n",
    "                        * padding_to_add,  # padding if needed\n",
    "                        labels=(\n",
    "                            label[start:end]\n",
    "                            + [-100] * padding_to_add  # padding if needed\n",
    "                        ),  # -100 is a special token for padding of labels,\n",
    "                        attention_masks=(\n",
    "                            encoding.attention_mask[start:end]\n",
    "                            + [0]\n",
    "                            * padding_to_add  # 0'd attenetion masks where we added padding\n",
    "                        ),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training_examples)\n",
    "\n",
    "    def __getitem__(self, idx) -> TrainingExample:\n",
    "\n",
    "        return self.training_examples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingExample(input_ids=[1284, 8343, 1234, 1106, 2373, 1105, 2934, 1103, 4503, 3603, 27551, 9990, 1105, 1106, 1587, 1639], attention_masks=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], labels=[4, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "label_set = LabelSet(labels=[\"CLEntity\"])\n",
    "ds = TraingDataset(\n",
    "    data=json_data, tokenizer=tokenizer, label_set=label_set, tokens_per_batch=16\n",
    ")\n",
    "ex = ds[10]\n",
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class TraingingBatch:\n",
    "    def __getitem__(self, item):\n",
    "        return getattr(self, item)\n",
    "\n",
    "    def __init__(self, examples: List[TrainingExample]):\n",
    "        self.input_ids: torch.Tensor\n",
    "        self.attention_masks: torch.Tensor\n",
    "        self.labels: torch.Tensor\n",
    "        input_ids: IntListList = []\n",
    "        masks: IntListList = []\n",
    "        labels: IntListList = []\n",
    "        for ex in examples:\n",
    "            input_ids.append(ex.input_ids)\n",
    "            masks.append(ex.attention_masks)\n",
    "            labels.append(ex.labels)\n",
    "        self.input_ids = torch.LongTensor(input_ids)\n",
    "        self.attention_masks = torch.LongTensor(masks)\n",
    "        self.labels = torch.LongTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers import BertForTokenClassification, AdamW, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\", num_labels=len(ds.label_set.ids_to_label.values())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./trainning_output/output1.txt\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=4,  # batch size per device during training\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    ds,\n",
    "    collate_fn=TraingingBatch,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3482,   117,  1105, 20421,  1142,  3438,  1111,  2174,  8225,   119,\n",
      "           147,  9272,  9565,  1110,  1226,  1104],\n",
      "        [16664,  2414,  1105,  1150,  1138,  1680,  1103,  2016,  1104,  1103,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [ 5424,   119,  1130, 12371,   117,  1103, 24705, 20937,  1389,  4924,\n",
      "          1867,  1104,  1103,     0,     0,     0],\n",
      "        [13915,  1112,  1103,  4503,  3603, 27551,   117,  1142,  1383,  1104,\n",
      "          1759,  1511,  1844,  4237,   117,  2371]])\n",
      "tensor([[   4,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, -100, -100],\n",
      "        [   4,    0,    1,    2,    3,    0,    4,    0,    1,    3,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [   1,    2,    2,    2,    3,    0,    0,    0,    0,    0,    4, -100,\n",
      "         -100, -100, -100, -100],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         -100, -100, -100, -100]])\n",
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(list(dataloader)[0].input_ids)\n",
    "print(list(dataloader)[0].labels)\n",
    "print(len(list(dataloader)[0].input_ids[0]))\n",
    "print(len(list(dataloader)[0].labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6521, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5041, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5877, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4045, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2162, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3801, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2443, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3830, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0788, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3048, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0057, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0927, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8509, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9573, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0834, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7541, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9077, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5362, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6874, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8043, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0622, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7217, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for num, batch in enumerate(dataloader):\n",
    "    \n",
    "    output = model(\n",
    "        input_ids=batch.input_ids,\n",
    "        attention_mask=batch.attention_masks,\n",
    "        labels=batch.labels,\n",
    "    )\n",
    "    print(output.loss)\n",
    "    output.loss.backward()\n",
    "    optimizer.step()\n",
    "    if num > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda (base)",
   "language": "python",
   "name": "anaconda-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
