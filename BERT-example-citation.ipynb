{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r'Iliad\\s\\d{1,2}\\.\\d{1,4}|Il\\.*\\s\\d{1,2}\\.\\d{1,4}|Iliad\\s.[ivxlcdm]*\\.\\s*\\d{1,4}|Il\\.*\\s.[ivxlcdm]*\\.\\s*\\d{1,4}|book\\s*.[ivxlcdm]\\.\\sline\\s*\\d{1,4}'\n",
    "re.compile(pattern, re.IGNORECASE)\n",
    "\n",
    "def get_annotations(text, pattern):\n",
    "    annotations = []\n",
    "    for match in re.finditer(pattern, text):\n",
    "        label_dic = dict()\n",
    "        label_dic['start'] = match.start()\n",
    "        label_dic['end'] = match.end()\n",
    "        label_dic['label'] = 'Citation' # Entity starting with a capital letter\n",
    "        annotations.append(label_dic)\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'STOP', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Early Journal Content on JSTOR, Free to Anyone in the World', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'This article is one of nearly 500,000 scholarly works digitized and made freely available to everyone in', 'annotations': []}, {'content': 'the world by JSTOR.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Known as the Early Journal Content, this set of works include research articles, news, letters, and other', 'annotations': []}, {'content': 'writings published in more than 200 of the oldest leading academic journals. The works date from the', 'annotations': []}, {'content': 'mid-seventeenth to the early twentieth centuries.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'We encourage people to read and share the Early Journal Content openly and to tell others that this', 'annotations': []}, {'content': 'resource exists. People may post this content online or redistribute in any way for non-commercial', 'annotations': []}, {'content': 'purposes.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Read more about Early Journal Content at http://about.jstor.org/participate-jstor/individuals/early-', 'annotations': []}, {'content': 'journal-content .', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'JSTOR is a digital library of academic journals, books, and primary source objects. JSTOR helps people', 'annotations': []}, {'content': 'discover, use, and build upon a wide range of content through a powerful research and teaching', 'annotations': []}, {'content': 'platform, and preserves this content for future generations. JSTOR is part of ITHAKA, a not-for-profit', 'annotations': []}, {'content': 'organization that also includes Ithaka S+R and Portico. For more information about JSTOR, please', 'annotations': []}, {'content': 'contact support@jstor.org.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'April 25, 1921)', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'THE CLASSICAL. WEEKLY', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '183', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'reveals a long, loving, and intimate association with', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'the best Latin masters, which, when combined with', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'sound critical sense, commends their literary judgments.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Union College,', 'annotations': []}, {'content': 'Schenectady, N. Y.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'George Dwight Kellogg.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Thucydides, Book IV, Chapters I-XLI (Pylus and', 'annotations': []}, {'content': 'Sphacteria). Edited by J. H. E. Crees and J. C.', 'annotations': []}, {'content': 'Wordsworth. Cambridge: at the University Press', 'annotations': []}, {'content': '(1919). Pp. xvi + 96.', 'annotations': []}, {'content': 'In a brief Preface it is stated that this edition has', 'annotations': []}, {'content': 'been prepared for those who have not long been study-', 'annotations': []}, {'content': 'ing Greek and who have reached the stage of the', 'annotations': []}, {'content': '\"First School Examination \" . A vocabulary has therefore', 'annotations': []}, {'content': 'been added (72-96). The book contains also a map of', 'annotations': []}, {'content': 'Pylus and Sphacteria. The Introduction (ix-xv) deals', 'annotations': []}, {'content': 'with the life and work of Thucydides and the contents', 'annotations': []}, {'content': 'of his history, Book IV, Chapters I-XLI. Mr. Crees,', 'annotations': []}, {'content': 'author of the Introduction, writes enthusiastically of', 'annotations': []}, {'content': \"Thucydides as a historian. Thucydides's history, he\", 'annotations': []}, {'content': 'says, \"would at any time have been a great work, but', 'annotations': []}, {'content': 'for its date it is in its conception a marvellous achieve-', 'annotations': []}, {'content': 'ment, and the expression of a person lity which', 'annotations': []}, {'content': 'compels respect\". Thucydides necessarily, as a true', 'annotations': []}, {'content': 'Athenian, was a partisan, but he was none the less able', 'annotations': []}, {'content': 'to efface his partisanship and \"achieved a monumental', 'annotations': []}, {'content': 'impartiality\". Thucydides, the aristocrat, is so fair to', 'annotations': []}, {'content': 'Cleon that \"the champions of Cleon must, and can', 'annotations': []}, {'content': 'base their championship on the evidence of Thucydi-', 'annotations': []}, {'content': 'des\". There are 38 pages of notes to 33 of the Greek', 'annotations': []}, {'content': 'text. Yet, I suspect, in more than one place, the stu-', 'annotations': []}, {'content': 'dent who has not \"long been studying Greek\" would', 'annotations': []}, {'content': 'need more assistance than the authors give him toward', 'annotations': []}, {'content': 'the interpretation of the text. c. K.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'A NOTE ON THE RED RAIN IN ILIAD 16.459', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'While listening recently to some lectures of Professor', 'annotations': []}, {'content': 'David M. Robinson on Homer, I was led to ask: Are', 'annotations': []}, {'content': 'the following passages of the Iliad merely the product', 'annotations': []}, {'content': \"of the poet's imagination or do they refer to real\", 'annotations': []}, {'content': 'natural phenomena?', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'In Iliad 16.459 we read 1', 'annotations': [{'start': 3, 'end': 15, 'label': 'Citation'}]}, {'content': '', 'annotations': []}, {'content': 'She spoke nor did the sire of Gods and men', 'annotations': []}, {'content': 'Unheeding hear, but poured down on the earth', 'annotations': []}, {'content': 'Rain drops of blood, so honoring his dear son,', 'annotations': []}, {'content': 'Him whom Patroclus was foredoomed to slay', 'annotations': []}, {'content': \"In Troy's rich soil far from his native land.\", 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'In Iliad 11.54 we read:', 'annotations': [{'start': 3, 'end': 14, 'label': 'Citation'}]}, {'content': '', 'annotations': []}, {'content': 'Zeus roused an evil blare of war and sent', 'annotations': []}, {'content': 'Down from high heaven his rain drops stained with', 'annotations': []}, {'content': 'blood.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Again in Hesiod, Shield of Heracles 383-385, in a', 'annotations': []}, {'content': 'passage perhaps imitated from the above, we have:', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Loud thundered Zeus, the counselor, flinging down', 'annotations': []}, {'content': 'From heaven bloody rain drops, setting thus', 'annotations': []}, {'content': 'A sign of battle to his great-souled son.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'In short, is there such a thing as red rain, apart from', 'annotations': []}, {'content': 'the effluvia of butterflies as suggested by Buchholz,', 'annotations': []}, {'content': 'Die Homerische Realien, 3.91, and is Homer justified', 'annotations': []}, {'content': 'in the use he makes of it? That there is and that Homer', 'annotations': []}, {'content': 'is better acquainted with and truer to nature than', 'annotations': []}, {'content': 'some of his critics is shown by the following note', 'annotations': []}, {'content': \"appended to certain verses of John Ruskin's Poem,\", 'annotations': []}, {'content': 'The Broken Chain (Geo. Allen, Library Edition of', 'annotations': []}, {'content': 'John Ruskin, 2.177 [1903I). The verses are:', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Like purple-rain at evening shed', 'annotations': []}, {'content': \"On Sestri's cedar-darkened shore.\", 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'The note runs thus:', 'annotations': []}, {'content': 'I never saw such a thing but once, on the mountains of', 'annotations': []}, {'content': 'Sestri in the Gulf of Genoa. The whole western half', 'annotations': []}, {'content': 'of the sky was one intense ambei colour, the air crystal-', 'annotations': []}, {'content': 'line and cloudless, the other half grey with drifting', 'annotations': []}, {'content': 'showers. At the instant of sunset, the whole mass of', 'annotations': []}, {'content': 'rain turned of a deep rose-colour, the consequent rain-', 'annotations': []}, {'content': 'bow being not varied with the seven colours, but one', 'annotations': []}, {'content': 'broad belt of paler rose; the other tints being so deli-', 'annotations': []}, {'content': 'cate as to be overwhelmed by the crimson of the rain.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'I have myself witnessed red rain in Chatham, Massa-', 'annotations': []}, {'content': 'chusetts, over Nantucket Sound. When one lives in', 'annotations': []}, {'content': 'the country and on the sea year in and year out, one', 'annotations': []}, {'content': 'acquires a wholesome respect for the observing powers', 'annotations': []}, {'content': 'of the classical poets, notably Homer. The phe-', 'annotations': []}, {'content': 'nomenon occurred at sunset, with drifting curtains of', 'annotations': []}, {'content': 'rain between the observer and the sun. These the red', 'annotations': []}, {'content': 'rays of the sinking orb shot through and through with', 'annotations': []}, {'content': 'deep crimson that faded and revived as the curtains of', 'annotations': []}, {'content': 'rain fell and succeeded one another. There was no', 'annotations': []}, {'content': 'rainbow, as the rain was between the observer and the', 'annotations': []}, {'content': 'sun, for one always sees a rainbow when he is between', 'annotations': []}, {'content': 'the rain and the sun (or the moon, in the case of a lunar', 'annotations': []}, {'content': 'rainbow, which is very rare), and of course the luminary', 'annotations': []}, {'content': 'cannot be very high in the heavens in either case.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': \"Given such a phenomenon, Homer's application is\", 'annotations': []}, {'content': 'obvious and justified, as the following quotations from', 'annotations': []}, {'content': 'Byron, Sardanapalus, and Turner, Fallacies of Hope,', 'annotations': []}, {'content': 'prove. In Byron, the Chaldean priest says of the', 'annotations': []}, {'content': 'sinking sun:', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'How red he glares amidst those deepening clouds,', 'annotations': []}, {'content': 'Like the blood he predicts.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': \"Turner's lines were over a picture of The Fall of\", 'annotations': []}, {'content': 'Carthage:', 'annotations': []}, {'content': '', 'annotations': []}, {'content': \"While o'er the western wave the ensanguined sun\", 'annotations': []}, {'content': 'Is gathering huge a stormy signal spread,', 'annotations': []}, {'content': 'And set portentous.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Rogers Fellow in Classics,', 'annotations': []}, {'content': 'The Johns Hopkins University.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Carol Wight.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': \"'The translations are my own. So are the italics in the passages\", 'annotations': []}, {'content': 'quoted.', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'CICERO, CATILINAM 2.4, ITERUM', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Utinam ille omnis secum suas copias eduxisset!', 'annotations': []}, {'content': 'Tongilium mihi eduxit, quern amare in praetexta', 'annotations': []}, {'content': 'coeperat; Publicium et Minucium, quorum aes', 'annotations': []}, {'content': 'alienum contractum in popina nullum rei publicae', 'annotations': []}, {'content': 'motum afferre poterat, reliquit. Quos viros! quanto', 'annotations': []}, {'content': 'aerealieno! quam valentis! quamnobilis!', 'annotations': []}, {'content': '', 'annotations': []}, {'content': 'Until Professor Herrouet endeavored, in The', 'annotations': []}, {'content': 'Classical Weekly 14. 87, to refute my punctuation', 'annotations': []}, {'content': '', 'annotations': []}, {'content': '', 'annotations': []}]\n"
     ]
    }
   ],
   "source": [
    "json_data = []\n",
    "book = open(\"./example-texts/red-rain-in-iliad.txt\")\n",
    "for line in book:\n",
    "    line = line.strip()\n",
    "    \n",
    "    line_data = dict()\n",
    "    line_data['content'] = line\n",
    "    line_data['annotations'] = get_annotations(line, pattern)\n",
    "    json_data.append(line_data)\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast,  BatchEncoding\n",
    "from tokenizers import Encoding\n",
    "def align_tokens_and_annotations_bilou(tokenized: Encoding, annotations):\n",
    "    tokens = tokenized.tokens\n",
    "    aligned_labels = [\"O\"] * len(\n",
    "        tokens\n",
    "    )  # Make a list to store our labels the same length as our tokens\n",
    "    for anno in annotations:\n",
    "        annotation_token_ix_set = (\n",
    "            set()\n",
    "        )  # A set that stores the token indices of the annotation\n",
    "        for char_ix in range(anno[\"start\"], anno[\"end\"]):\n",
    "\n",
    "            token_ix = tokenized.char_to_token(char_ix)\n",
    "            if token_ix is not None:\n",
    "                annotation_token_ix_set.add(token_ix)\n",
    "        if len(annotation_token_ix_set) == 1:\n",
    "            # If there is only one token\n",
    "            token_ix = annotation_token_ix_set.pop()\n",
    "            prefix = (\n",
    "                \"U\"  # This annotation spans one token so is prefixed with U for unique\n",
    "            )\n",
    "            aligned_labels[token_ix] = f\"{prefix}-{anno['label']}\"\n",
    "\n",
    "        else:\n",
    "\n",
    "            last_token_in_anno_ix = len(annotation_token_ix_set) - 1\n",
    "            for num, token_ix in enumerate(sorted(annotation_token_ix_set)):\n",
    "                if num == 0:\n",
    "                    prefix = \"B\"\n",
    "                elif num == last_token_in_anno_ix:\n",
    "                    prefix = \"L\"  # Its the last token\n",
    "                else:\n",
    "                    prefix = \"I\"  # We're inside of a multi token annotation\n",
    "                aligned_labels[token_ix] = f\"{prefix}-{anno['label']}\"\n",
    "    return aligned_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] - O\n",
      "In - O\n",
      "Il - B-Citation\n",
      "##ia - I-Citation\n",
      "##d - I-Citation\n",
      "16 - I-Citation\n",
      ". - I-Citation\n",
      "45 - I-Citation\n",
      "##9 - L-Citation\n",
      "we - O\n",
      "read - O\n",
      "1 - O\n",
      "[SEP] - O\n"
     ]
    }
   ],
   "source": [
    "example = {'content': 'In Iliad 16.459 we read 1', 'annotations': [{'start': 3, 'end': 15, 'label': 'Citation'}]}\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased') # Load a pre-trained tokenizer\n",
    "tokenized_batch : BatchEncoding = tokenizer(example[\"content\"])\n",
    "tokenized_text : Encoding = tokenized_batch[0]\n",
    "labels = align_tokens_and_annotations_bilou(tokenized_text, example[\"annotations\"])\n",
    "for token, label in zip(tokenized_text.tokens, labels):\n",
    "    print(token, \"-\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List,Any\n",
    "IntList = List[int] # A list of token_ids\n",
    "IntListList = List[IntList] # A List of List of token_ids, e.g. a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "class LabelSet:\n",
    "    def __init__(self, labels: List[str]):\n",
    "        self.labels_to_id = {}\n",
    "        self.ids_to_label = {}\n",
    "        self.labels_to_id[\"O\"] = 0\n",
    "        self.ids_to_label[0] = \"O\"\n",
    "        num = 0  # in case there are no labels\n",
    "        # Writing BILU will give us incremntal ids for the labels\n",
    "        for _num, (label, s) in enumerate(itertools.product(labels, \"BILU\")):\n",
    "            num = _num + 1  # skip 0\n",
    "            l = f\"{s}-{label}\"\n",
    "            self.labels_to_id[l] = num\n",
    "            self.ids_to_label[num] = l\n",
    "        # Add the OUTSIDE label - no label for the token\n",
    "\n",
    "    def get_aligned_label_ids_from_annotations(self, tokenized_text, annotations):\n",
    "        raw_labels = align_tokens_and_annotations_bilou(tokenized_text, annotations)\n",
    "        return list(map(self.labels_to_id.get, raw_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingExample:\n",
    "    input_ids: IntList\n",
    "    attention_masks: IntList\n",
    "    labels: IntList\n",
    "\n",
    "\n",
    "class TraingDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: Any,\n",
    "        label_set: LabelSet,\n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        tokens_per_batch=32,\n",
    "        window_stride=None,\n",
    "    ):\n",
    "        self.label_set = label_set\n",
    "        if window_stride is None:\n",
    "            self.window_stride = tokens_per_batch\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = []\n",
    "        self.annotations = []\n",
    "\n",
    "        for example in data:\n",
    "            self.texts.append(example[\"content\"])\n",
    "            self.annotations.append(example[\"annotations\"])\n",
    "        ###TOKENIZE All THE DATA\n",
    "        tokenized_batch = self.tokenizer(self.texts, add_special_tokens=False)\n",
    "        ###ALIGN LABELS ONE EXAMPLE AT A TIME\n",
    "        aligned_labels = []\n",
    "        for ix in range(len(tokenized_batch.encodings)):\n",
    "            encoding = tokenized_batch.encodings[ix]\n",
    "            raw_annotations = self.annotations[ix]\n",
    "            aligned = label_set.get_aligned_label_ids_from_annotations(\n",
    "                encoding, raw_annotations\n",
    "            )\n",
    "            aligned_labels.append(aligned)\n",
    "        ###END OF LABEL ALIGNMENT\n",
    "\n",
    "        ###MAKE A LIST OF TRAINING EXAMPLES. (This is where we add padding)\n",
    "        self.training_examples: List[TrainingExample] = []\n",
    "        empty_label_id = \"O\"\n",
    "        for encoding, label in zip(tokenized_batch.encodings, aligned_labels):\n",
    "            length = len(label)  # How long is this sequence\n",
    "            for start in range(0, length, self.window_stride):\n",
    "\n",
    "                end = min(start + tokens_per_batch, length)\n",
    "\n",
    "                # How much padding do we need ?\n",
    "                padding_to_add = max(0, tokens_per_batch - end + start)\n",
    "                self.training_examples.append(\n",
    "                    TrainingExample(\n",
    "                        # Record the tokens\n",
    "                        input_ids=encoding.ids[start:end]  # The ids of the tokens\n",
    "                        + [self.tokenizer.pad_token_id]\n",
    "                        * padding_to_add,  # padding if needed\n",
    "                        labels=(\n",
    "                            label[start:end]\n",
    "                            + [-100] * padding_to_add  # padding if needed\n",
    "                        ),  # -100 is a special token for padding of labels,\n",
    "                        attention_masks=(\n",
    "                            encoding.attention_mask[start:end]\n",
    "                            + [0]\n",
    "                            * padding_to_add  # 0'd attenetion masks where we added padding\n",
    "                        ),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training_examples)\n",
    "\n",
    "    def __getitem__(self, idx) -> TrainingExample:\n",
    "\n",
    "        return self.training_examples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingExample(input_ids=[1284, 8343, 1234, 1106, 2373, 1105, 2934, 1103, 4503, 3603, 27551, 9990, 1105, 1106, 1587, 1639], attention_masks=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], labels=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "label_set = LabelSet(labels=[\"citation\"])\n",
    "ds = TraingDataset(\n",
    "    data=json_data, tokenizer=tokenizer, label_set=label_set, tokens_per_batch=16\n",
    ")\n",
    "ex = ds[10]\n",
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TraingingBatch:\n",
    "    def __getitem__(self, item):\n",
    "        return getattr(self, item)\n",
    "\n",
    "    def __init__(self, examples: List[TrainingExample]):\n",
    "        self.input_ids: torch.Tensor\n",
    "        self.attention_masks: torch.Tensor\n",
    "        self.labels: torch.Tensor\n",
    "        input_ids: IntListList = []\n",
    "        masks: IntListList = []\n",
    "        labels: IntListList = []\n",
    "        for ex in examples:\n",
    "            input_ids.append(ex.input_ids)\n",
    "            masks.append(ex.attention_masks)\n",
    "            labels.append(ex.labels)\n",
    "        self.input_ids = torch.LongTensor(input_ids)\n",
    "        self.attention_masks = torch.LongTensor(masks)\n",
    "        self.labels = torch.LongTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-7dc2396a1faa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     )\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\", num_labels=len(ds.label_set.ids_to_label.values())\n",
    ")\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    ds,\n",
    "    collate_fn=TraingingBatch,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    ")\n",
    "for num, batch in enumerate(dataloader):\n",
    "    loss, logits = model(\n",
    "        input_ids=batch.input_ids,\n",
    "        attention_mask=batch.attention_masks,\n",
    "        labels=batch.labels,\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)\n",
    "    if num > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda (base)",
   "language": "python",
   "name": "anaconda-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
